{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import numpy.random as rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(460, 5, 1, 30, 30)\n",
      "(460, 5)\n"
     ]
    }
   ],
   "source": [
    "labels=[]\n",
    "images=[]\n",
    "for i in range(460):\n",
    "    m=i+1\n",
    "    samples_num=os.listdir(os.path.join('D:/数据库/palmdata/iitd',str(m)))\n",
    "    imgs=[]\n",
    "    labs=[]\n",
    "    for k in range(5):  #len(samples_num)\n",
    "        j=k+1\n",
    "        tpath=os.path.join(r'D:/数据库/palmdata/iitd',str(m),str(j)+'.jpeg')     #路径(/home/ouc/river/test)+图片名（img_m）\n",
    "        fopen = Image.open(tpath)\n",
    "        transform=transforms.Compose([transforms.Resize((img_size, img_size)),  # 将图片缩放到指定大小（h,w）或者保持长宽比并缩放最短的边到int大小\n",
    "                                         transforms.Grayscale(),\n",
    "                                         transforms.ToTensor()]) \n",
    "        data=np.array(transform(fopen))#data就是预处理后，可以送入模型进行训练的数据了\n",
    "        imgs.append(data)\n",
    "        labs.append(m)\n",
    "    labels.append(labs)\n",
    "    images.append(imgs)\n",
    "\n",
    "labels=np.array(labels)\n",
    "images=np.array(images)\n",
    "print(images.shape) #(460,5)\n",
    "print(labels.shape)   #(460,5,1, 28, 28)\n",
    "\n",
    "# train_x=images[:2025]   #前360类数据作为训练集\n",
    "# train_y=labels[:2025]\n",
    "# val_x=images[2025:2600]  #后100类数据作为测试集\n",
    "# val_y=labels[2025:2600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练集\n",
    "x_train=images[:360]\n",
    "y_train=labels[:360]\n",
    "#测试集\n",
    "x_test=images[360:]\n",
    "y_test=labels[360:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成一个元batch数据，包含8个任务，每个任务输入batch_size=64的三元组\n",
    "base_batch_size=80\n",
    "num_tasks=8\n",
    "def make_cache(images,labels,name='train'):\n",
    "    classes,counts = np.unique(labels, return_counts=True)\n",
    "    data_cache = [] \n",
    "    for _ in range(10): #1个epoch包含10个元batch\n",
    "        tasks=[]  #保存一个元batch的数据，长度是num_tasks\n",
    "        for _ in range(num_tasks):  #生成8个任务\n",
    "            task=[] #保存一个任务的数据，长度是base_batch_size\n",
    "            for i in range(base_batch_size):\n",
    "                selected_cls = rng.choice(classes)   #选择第一个样本的类   #1~360,361~460\n",
    "                genuine_imposter_ratio=random.randint(0,1)\n",
    "                if genuine_imposter_ratio: #正例\n",
    "                    cls_2=selected_cls\n",
    "                else:  #负例\n",
    "                    while True:\n",
    "                        cls_2=random.choice(classes)\n",
    "                        if cls_2!=selected_cls:\n",
    "                            break\n",
    "                ex1, ex2 = rng.choice(5,replace=False,size=(2,))   #0~4\n",
    "                if name=='train':\n",
    "                    img1=images[selected_cls-1][ex1]\n",
    "                    img2=images[cls_2-1][ex2]\n",
    "                else:\n",
    "                    img1=images[selected_cls-361][ex1]\n",
    "                    img2=images[cls_2-361][ex2]\n",
    "                flag=genuine_imposter_ratio #返回0/1标记\n",
    "                task.append((img1,img2,flag))\n",
    "            task=np.array(task)\n",
    "            tasks.append(task)\n",
    "        tasks=np.array(tasks)\n",
    "        #print(tasks.shape)  #(8, 80, 3)\n",
    "        data_cache.append(tasks)\n",
    "    return data_cache    #(10, 8, 80, 3)\n",
    "\n",
    "#元网络的数据集\n",
    "datasets_cache = {\"train\": make_cache(x_train,y_train),\n",
    "                  \"test\": make_cache(x_test,y_test,name='test')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = {\"train\": 0, \"test\": 0}  #记录当前取到第几batch\n",
    "datasets={'train':x_train,'test':x_test}\n",
    "datasets_y={'train':y_train,'test':y_test}\n",
    "\n",
    "#迭代地取出一个个batch的数据集\n",
    "def next(mode='train'):\n",
    "    # update cache if indexes is larger than len(data_cache)\n",
    "    if indexes[mode] >= len(datasets_cache[mode]):\n",
    "        indexes[mode] = 0\n",
    "        datasets_cache[mode] = make_cache(datasets[mode],datasets[mode],name=mode)\n",
    "\n",
    "    next_batch = datasets_cache[mode][indexes[mode]]\n",
    "    indexes[mode] += 1\n",
    "\n",
    "    return next_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#=======================================模型\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from copy import deepcopy, copy\n",
    "\n",
    "#构建基学习器,学习一个任务,sp集（80，3），qu集（16，3）\n",
    "class BaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.vars = nn.ParameterList()  ## 包含了所有需要被优化的tensor参数w和b\n",
    "        self.vars_bn = nn.ParameterList()  ##bn层参数\n",
    "\n",
    "        # 第1个conv2d=================================\n",
    "        # in_channels = 1, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 1, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        # 第1个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        self.vars_bn.extend([running_mean, running_var])   #标准化？\n",
    "\n",
    "        # 第2个conv2d====================================\n",
    "        # in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        # 第2个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    "\n",
    "        # 第3个conv2d====================================\n",
    "        # in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        # 第3个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    "\n",
    "        # 第4个conv2d======================================\n",
    "        # in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        # 第4个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    "\n",
    "        ##linear===========================================\n",
    "        weight = nn.Parameter(torch.ones([5, 64]))\n",
    "        bias = nn.Parameter(torch.zeros(5))\n",
    "        self.vars.extend([weight, bias])\n",
    "        \n",
    "        ##联合两个输入的fc层\n",
    "        weight = nn.Parameter(torch.ones([10, 64]))\n",
    "        bias = nn.Parameter(torch.zeros(10))\n",
    "        self.vars.extend([weight, bias])\n",
    "        \n",
    "        weight = nn.Parameter(torch.ones([64, 1]))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "        \n",
    "\n",
    "    #         self.conv = nn.Sequential(\n",
    "    #             nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2),\n",
    "    #             nn.BatchNorm2d(64),\n",
    "    #             nn.ReLU(),\n",
    "    #             nn.MaxPool2d(2),\n",
    "\n",
    "    #             nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2),\n",
    "    #             nn.BatchNorm2d(64),\n",
    "    #             nn.ReLU(),\n",
    "    #             nn.MaxPool2d(2),\n",
    "\n",
    "    #             nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2),\n",
    "    #             nn.BatchNorm2d(64),\n",
    "    #             nn.ReLU(),\n",
    "    #             nn.MaxPool2d(2),\n",
    "\n",
    "    #             nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2),\n",
    "    #             nn.BatchNorm2d(64),\n",
    "    #             nn.ReLU(),\n",
    "    #             nn.MaxPool2d(2),\n",
    "\n",
    "    #             FlattenLayer(),\n",
    "    #             nn.Linear(64,5)\n",
    "    #         )\n",
    "\n",
    "    def forward(self, x1,x2, params=None, bn_training=True):\n",
    "        '''\n",
    "        :bn_training: set False to not update\n",
    "        :return:\n",
    "        '''\n",
    "        if params is None:\n",
    "            params = self.vars\n",
    "        x=[x1,x2]\n",
    "        for i in range(2):\n",
    "            weight, bias = params[0], params[1]  # 第1个CONV层\n",
    "            x[i] = F.conv2d(x[i], weight, bias, stride=2, padding=2)\n",
    "            weight, bias = params[2], params[3]  # 第1个BN层\n",
    "            running_mean, running_var = self.vars_bn[0], self.vars_bn[1]\n",
    "            x[i] = F.batch_norm(x[i], running_mean, running_var, weight=weight, bias=bias, training=bn_training)\n",
    "            x[i] = F.max_pool2d(x[i], kernel_size=2)  # 第1个MAX_POOL层\n",
    "            x[i] = F.relu(x[i], inplace=[True])  # 第1个relu\n",
    "            \n",
    "            weight, bias = params[4], params[5]  # 第2个CONV层\n",
    "            x[i] = F.conv2d(x[i], weight, bias, stride=2, padding=2)\n",
    "            weight, bias = params[6], params[7]  # 第2个BN层\n",
    "            running_mean, running_var = self.vars_bn[2], self.vars_bn[3]\n",
    "            x[i]= F.batch_norm(x[i], running_mean, running_var, weight=weight, bias=bias, training=bn_training)\n",
    "            x[i]= F.max_pool2d(x[i], kernel_size=2)  # 第2个MAX_POOL层\n",
    "            x[i]= F.relu(x[i], inplace=[True])  # 第2个relu\n",
    "            \n",
    "            weight, bias = params[8], params[9]  # 第3个CONV层\n",
    "            x[i] = F.conv2d(x[i], weight, bias, stride=2, padding=2)\n",
    "            weight, bias = params[10], params[11]  # 第3个BN层\n",
    "            running_mean, running_var = self.vars_bn[4], self.vars_bn[5]\n",
    "            x[i]= F.batch_norm(x[i], running_mean, running_var, weight=weight, bias=bias, training=bn_training)\n",
    "            x[i]= F.max_pool2d(x[i], kernel_size=2)  # 第3个MAX_POOL层\n",
    "            x[i]= F.relu(x[i], inplace=[True])  # 第3个relu\n",
    "            \n",
    "            weight, bias = params[12], params[13]  # 第4个CONV层\n",
    "            x[i] = F.conv2d(x[i], weight, bias, stride=2, padding=2)\n",
    "            x[i] = F.relu(x[i], inplace=[True])  # 第4个relu\n",
    "            weight, bias = params[14], params[15]  # 第4个BN层\n",
    "            running_mean, running_var = self.vars_bn[6], self.vars_bn[7]\n",
    "            x[i]= F.batch_norm(x[i], running_mean, running_var, weight=weight, bias=bias, training=bn_training)\n",
    "            x[i]= F.max_pool2d(x[i], kernel_size=2)  # 第4个MAX_POOL层\n",
    "            \n",
    "            x[i]= x[i].view(x[i].size(0), -1)  ## flatten\n",
    "            weight, bias = params[16], params[17]  # linear\n",
    "            x[i] = F.linear(x[i], weight, bias)\n",
    "            \n",
    "        x_two=torch.cat((x[0],x[1]),1)  \n",
    "        weight, bias = params[18], params[19]   #联合两个输入的第一个fc层\n",
    "        output=F.linear(x_two, weight, bias)\n",
    "        output= F.relu(output, inplace=[True])\n",
    "    \n",
    "        weight, bias = params[20], params[21] \n",
    "        output=F.linear(output, weight, bias)\n",
    "        output= F.sigmoid(output, inplace=[True])\n",
    "        return output  #输出0/1\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#构建元学习器，学习一个batch任务（8个任务为一个batch）\n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.update_step = 5  ## 基学习器训练更新5次\n",
    "        self.update_step_test = 5\n",
    "        self.net = BaseNet().cuda()\n",
    "        self.meta_lr = 2e-4  #元学习器的学习率\n",
    "        self.base_lr = 4 * 1e-2  #基学习器的学习率\n",
    "        self.meta_optim = torch.optim.Adam(self.net.parameters(), lr=self.meta_lr)  #元学习器的优化器\n",
    "\n",
    "    def forward(datasets_cache):  #输入一个batch的数据=8个任务的数据，每执行一次这个函数，元学习器就更新一次  （8，80，3）\n",
    "        #x_spt, y_spt, x_qry, y_qry=x_spt,torch.LongTensor(y_spt),torch.LongTensor(x_qry),torch.LongTensor(y_qry)\n",
    "        # 初始化\n",
    "        # 8         80              3\n",
    "        task_num, base_batch_size, trip = datasets_cache.size()\n",
    "    \n",
    "        sp=datasets_cache[:][:64][:]\n",
    "        qu=datasets_cache[:][64:][:]\n",
    "        \n",
    "        loss_list_qu = [0 for _ in range(self.update_step + 1)]   #[0,0,0,0,0,0]  #保存一个任务内，基学习器的初始损失加后面5次更新的损失\n",
    "        correct_list = [0 for _ in range(self.update_step + 1)]   #[0,0,0,0,0,0]  #保存一个任务内，基学习器的初始准确数加后面5次更新的准确数\n",
    "        #遍历一个batch内的每个任务，对每个任务训练更新基学习器5次\n",
    "        for i in range(task_num):\n",
    "            ## 一个任务上基学习器的第0步更新\n",
    "            y_hat = self.net(sp[i][:][0], sp[i][:][1],params=None, bn_training=True)  # (ways * shots, ways) 用支持集，基学习器学习某一个任务\n",
    "\n",
    "            loss = F.binary_cross_entropy(y_hat, sp[i][:][2]) #在支持集上，计算基学习器在某个任务的损失\n",
    "            grad = torch.autograd.grad(loss, self.net.parameters()) #计算该任务的梯度\n",
    "            tuples = zip(grad, self.net.parameters())  ## 将梯度和参数\\theta一一对应起来\n",
    "            # fast_weights这一步相当于求了一个\\theta - \\alpha*\\nabla(L)\n",
    "            fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], tuples))  #更新一次后的基学习器参数\n",
    "            # 在query集上测试，计算准确率\n",
    "            # 这一步使用更新前的数据\n",
    "            with torch.no_grad():\n",
    "                y_hat = self.net(qu[i][:][0], qu[i][:][1], self.net.parameters(), bn_training=True) #使用前一步的基学习器计算查询集上的损失和准确数\n",
    "                loss_qry = F.binary_cross_entropy(y_hat, qu[i][:][2])\n",
    "                loss_list_qry[0] += loss_qry\n",
    "                pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "                correct = torch.eq(pred_qry, y_qry[i]).sum().item()\n",
    "                correct_list[0] += correct\n",
    "\n",
    "            # 使用更新后的数据在query集上测试。\n",
    "            with torch.no_grad():\n",
    "                y_hat = self.net(qu[i][:][0], qu[i][:][1], fast_weights, bn_training=True)  #使用更新后的基学习器计算查询集上的损失和准确数\n",
    "                loss_qry = F.binary_cross_entropy(y_hat, qu[i][:][2])\n",
    "                loss_list_qry[1] += loss_qry\n",
    "                pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "                correct = torch.eq(pred_qry, y_qry[i]).sum().item()\n",
    "                correct_list[1] += correct\n",
    "\n",
    "            for k in range(1, self.update_step):   #再进行4次支持集上训练后更新基学习器的参数，并记录查询集上的验证损失和准确数\n",
    "                y_hat = self.net(sp[i][:][0], sp[i][:][1], params=fast_weights, bn_training=True)\n",
    "                loss = F.binary_cross_entropy(y_hat, sp[i][:][2])\n",
    "                grad = torch.autograd.grad(loss, fast_weights)\n",
    "                tuples = zip(grad, fast_weights)\n",
    "                fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], tuples))\n",
    "\n",
    "                y_hat = self.net(qu[i][:][0], qu[i][:][1], params=fast_weights, bn_training=True)\n",
    "                loss_qry = F.binary_cross_entropy(y_hat, qu[i][:][2])\n",
    "                loss_list_qry[k + 1] += loss_qry\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)\n",
    "                    correct = torch.eq(pred_qry, y_qry[i]).sum().item()\n",
    "                    correct_list[k + 1] += correct\n",
    "        #         print('hello')\n",
    "\n",
    "        #元学习器参数更新\n",
    "        loss_qry = loss_list_qry[-1] / task_num  #计算最后一次更新基学习器（第五次更新）上查询集的平均每个任务的损失（表示这个batch的损失）===》作为外部元学习器的一次损失，更新一次元学习器的参数\n",
    "        self.meta_optim.zero_grad()  # 梯度清零\n",
    "        loss_qry.backward() #\n",
    "        self.meta_optim.step()\n",
    "\n",
    "        #计算当前这步的元学习器学到的基学习器对于一个任务1+（5次更新）的对应的查询集平均准确率和损失\n",
    "        accs = np.array(correct_list) / (query_size * task_num)\n",
    "        loss = np.array(loss_list_qry) / (task_num)\n",
    "        return accs, loss  #返回每一步（基学习器在每个任务上更新共六步）的平均损失和准确数\n",
    "\n",
    "    #对MAML训练得到的基学习器测试准确率\n",
    "    def finetunning(self, task):  #输入一个任务的数据  (80,3)\n",
    "\n",
    "        correct_list = [0 for _ in range(self.update_step_test + 1)]   #[0,0,0,0,0,0]\n",
    "        sp=datasets_cache[:64][:]\n",
    "        qu=datasets_cache[64:][:]\n",
    "        \n",
    "        new_net = deepcopy(self.net)\n",
    "        y_hat = new_net(sp[:][0], sp[:][1])\n",
    "        loss = F.binary_cross_entropy(y_hat, sp[:][2])\n",
    "        grad = torch.autograd.grad(loss, new_net.parameters())\n",
    "        fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], zip(grad, new_net.parameters())))\n",
    "\n",
    "        # 在query集上测试，计算准确率\n",
    "        # 这一步使用更新前的基学习器参数\n",
    "        with torch.no_grad():\n",
    "            y_hat = new_net(qu[:][0], qu[:][1], params=new_net.parameters(), bn_training=True)\n",
    "            pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "            correct = torch.eq(pred_qry, y_qry).sum().item()\n",
    "            correct_list[0] += correct\n",
    "\n",
    "        # 使用更新后的基学习器参数。\n",
    "        with torch.no_grad():\n",
    "            y_hat = new_net(qu[:][0], qu[:][1], params=fast_weights, bn_training=True)\n",
    "            pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "            correct = torch.eq(pred_qry, y_qry).sum().item()\n",
    "            correct_list[1] += correct\n",
    "\n",
    "        for k in range(1, self.update_step_test):  #五步更新基学习器（这里是剩下四步）\n",
    "            y_hat = new_net(sp[:][0], sp[:][1], params=fast_weights, bn_training=True)\n",
    "            loss = F.binary_cross_entropy(y_hat, sp[:][2])\n",
    "            grad = torch.autograd.grad(loss, fast_weights)\n",
    "            fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "            y_hat = new_net(qu[:][0], qu[:][1], fast_weights, bn_training=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_qry, y_qry).sum().item()\n",
    "                correct_list[k + 1] += correct\n",
    "\n",
    "        del new_net\n",
    "        accs = np.array(correct_list) / query_size  #返回一个任务上基学习器的平均准确率\n",
    "        return accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: double, float, float16, int64, int32, and uint8.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a15e9eb0a347>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;31m#print(type(datasets_cache))   #<class 'numpy.ndarray'>\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;31m#datasets_cache= torch.from_numpy(datasets_cache).to(device)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mdatasets_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mdatasets_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mdatasets_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatasets_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: double, float, float16, int64, int32, and uint8."
     ]
    }
   ],
   "source": [
    "import time\n",
    "device = torch.device('cuda')\n",
    "\n",
    "meta = MetaLearner().to(device)  #初始化元学习器\n",
    "\n",
    "epochs =1000 # 60000\n",
    "for step in range(epochs):\n",
    "    start = time.time()\n",
    "    datasets_cache = next('train')  #取出一个batch的数据（包含8个任务）\n",
    "    #print(type(datasets_cache))   #<class 'numpy.ndarray'>\n",
    "    #datasets_cache= torch.from_numpy(datasets_cache).to(device)\n",
    "    datasets_cache[:][:][0]= torch.from_numpy(datasets_cache[:][:][0]).to(device)\n",
    "    datasets_cache[:][:][1]= torch.from_numpy(datasets_cache[:][:][1]).to(device)\n",
    "    datasets_cache[:][:][2]=torch.from_numpy(datasets_cache[:][:][2]).to(device)\n",
    "#     print(type(datasets_cache[0][0]))\n",
    "#     print(datasets_cache[0][0][0].shape)  #(1, 30, 30)\n",
    "#     print(datasets_cache[0][0][1].shape)  #(1, 30, 30)\n",
    "#     print(datasets_cache[0][0][2])   #0/1\n",
    "\n",
    "\n",
    "\n",
    "    datasets_cache[:][:][:2]= torch.from_numpy(datasets_cache[:][:][:2]).to(device)\n",
    "    datasets_cache[:][:][2]=torch.from_numpy(datasets_cache[:][:][2]).to(device)\n",
    "                                 \n",
    "    accs, loss = meta(datasets_cache)  #更新一次元学习器，返回这一次在八个任务上基学习器每一步更新得到的的损失和准确率\n",
    "    end = time.time()\n",
    "\n",
    "    if step % 20 == 0:  #每更新100次元学习器，输出一次当前的基学习器准确率和损失情况\n",
    "        print(\"epoch:\", step)\n",
    "        print(accs)\n",
    "        print(loss)\n",
    "\n",
    "    if step % 30 == 0:  #每更新1000次元学习器，从测试集分别取出一个batch数据，从中依此取出一个任务的数据测试当前学到的基学习器\n",
    "        accs = []\n",
    "        for _ in range(1000 // task_num):\n",
    "            # db_train.next('test')\n",
    "            datasets_cache = next('test')\n",
    "            datasets_cache[:][:][0]= torch.from_numpy(datasets_cache[:][:][0]).to(device)\n",
    "            datasets_cache[:][:][1]= torch.from_numpy(datasets_cache[:][:][1]).to(device)\n",
    "            datasets_cache[:][:][2]=torch.from_numpy(datasets_cache[:][:][2]).to(device)\n",
    "\n",
    "            for task in datasets_cache:\n",
    "                test_acc = meta.finetunning(task)\n",
    "                accs.append(test_acc)\n",
    "        print('在mean process之前：', np.array(accs).shape)\n",
    "        accs = np.array(accs).mean(axis=0).astype(np.float16)\n",
    "        print('测试集准确率:', accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试集准确率: [0.2072 0.8696 0.9287 0.939  0.9414 0.942 ]  800个epoch，tju1/tju2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试集准确率: [0.2017 0.8677 0.9136 0.9175 0.9185 0.922 ]  1000个epoch tju1分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyenv] *",
   "language": "python",
   "name": "conda-env-pyenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
