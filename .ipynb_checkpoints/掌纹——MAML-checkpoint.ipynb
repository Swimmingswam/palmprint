{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def read_img(img_path):\n",
    "    #inp=np.array(Image.open('D:/数据库/palmdata/tju1/1/1.jpeg').resize((28,28)))\n",
    "    #inp=np.array(Image.open('D:/数据库/palmdata/tju1/1/1.jpeg')\n",
    "    transform = transforms.Compose([lambda img: Image.open(img_path).convert('L'),  #转成灰度图\n",
    "                                    lambda img: img.resize((28, 28)),  #统一大小\n",
    "                                    lambda img: np.reshape(img, (28, 28, 1)),\n",
    "                                    lambda img: np.transpose(img, [2, 0, 1]),   #通道在前\n",
    "                                    lambda img: img / 255.  #归一化\n",
    "                                    ])\n",
    "    inp = transform(img_path)  #图像处理\n",
    "    return inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(data_path):\n",
    "    img_items = []\n",
    "    files = os.listdir(data_path)\n",
    "    files=list(map(int,files))\n",
    "    files.sort()\n",
    "    files=list(map(str,files))\n",
    "    #print(files)\n",
    "    total_images=[]\n",
    "    for file in files:  #1~600\n",
    "        f_images=[]\n",
    "        images=os.listdir(os.path.join(data_path,file))\n",
    "        for i in range(1,len(images)+1): #1~10\n",
    "            image_path=os.path.join(data_path,file,str(i)+'.jpeg')\n",
    "            image=read_img(image_path)\n",
    "            f_images.append(image)\n",
    "        total_images.append(f_images)\n",
    "    total_images=torch.from_numpy(np.array(total_images)).unsqueeze(2)\n",
    "    return total_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DB: train (600, 10, 1, 1, 28, 28) test (600, 10, 1, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "# train_path='D:/数据库/palmdata/tju1'\n",
    "# test_path='D:/数据库/palmdata/tju2'\n",
    "\n",
    "train_path='D:/数据库/TJU1/train'\n",
    "test_path='D:/数据库/TJU1/test'\n",
    "\n",
    "x_train=get_images(train_path).numpy()\n",
    "x_test=get_images(test_path).numpy()\n",
    "\n",
    "datasets={'train':x_train,'test':x_test}\n",
    "\n",
    "print(\"DB: train\", x_train.shape, \"test\", x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成一个batch的数据，一个batch包含8个N way k shot 任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_way = 5\n",
    "k_spt = 1  ## support data 的个数\n",
    "k_query = 5  ## query data 的个数\n",
    "imgsz = 28  ##图片边长\n",
    "resize = imgsz\n",
    "task_num = 8\n",
    "batch_size = task_num  #一个batch包含8个任务，每个任务都是（s集+q集）？\n",
    "indexes = {\"train\": 0, \"test\": 0}  #记录当前取到第几batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#生成一个epoch的模型的训练集或测试集列表，[support_set_x, support_set_y, target_x, target_y]\n",
    "def load_data_cache(dataset):\n",
    "    #  take 5 way 1 shot as example: 5 * 1\n",
    "    setsz = k_spt * n_way   #5way*1shot\n",
    "    querysz = k_query * n_way  #5way*15shot\n",
    "    data_cache = []  #（10，8，[x_spts, y_spts, x_qrys, y_qrys]）  #存储一个epoch的数据集，一个epoch包含10个batch，每个batch包含8个N way K shot 任务\n",
    "\n",
    "    # print('preload next 10 caches of batch_size of batch.')\n",
    "    for sample in range(10):  # 遍历一个epoch，生成10个batch的任务组\n",
    "\n",
    "        x_spts, y_spts, x_qrys, y_qrys = [], [], [], []  #1个batch=8个任务的数据\n",
    "        for i in range(batch_size):  # 遍历每个batch任务组，生成8个任务，1个任务=1支持集（5*1）+1查询集（5*15）\n",
    "\n",
    "            x_spt, y_spt, x_qry, y_qry = [], [], [], []  #一个任务的数据\n",
    "            #随机从1200或423类中选出n_way给类别\n",
    "            selected_cls = np.random.choice(dataset.shape[0], n_way, replace=False)\n",
    "\n",
    "            # 构造一个任务的support集和query集\n",
    "            for j, cur_class in enumerate(selected_cls):\n",
    "                #对于每个选出的类别，从20个样本中随机选出k_spt + k_query个样本\n",
    "                selected_img = np.random.choice(10, k_spt + k_query, replace=False)\n",
    "\n",
    "                x_spt.append(dataset[cur_class][selected_img[:k_spt]])\n",
    "                x_qry.append(dataset[cur_class][selected_img[k_spt:]])\n",
    "                y_spt.append([j for _ in range(k_spt)])\n",
    "                y_qry.append([j for _ in range(k_query)])\n",
    "\n",
    "            # np.random.permutation数据打乱，生成新数组\n",
    "            perm = np.random.permutation(n_way * k_spt)\n",
    "            \n",
    "            x_spt = np.array(x_spt).reshape(n_way * k_spt, 1, resize, resize)[perm]  #(5,1,28,28)\n",
    "            y_spt = np.array(y_spt).reshape(n_way * k_spt)[perm]  #(5)\n",
    "\n",
    "            perm = np.random.permutation(n_way * k_query)\n",
    "            x_qry = np.array(x_qry).reshape(n_way * k_query, 1, resize, resize)[perm]  #(15*5,1,28,28)\n",
    "            y_qry = np.array(y_qry).reshape(n_way * k_query)[perm]  #(75)\n",
    "\n",
    "            # append [sptsz, 1, 28, 28] => [batch_size, setsz, 1, 28, 28]\n",
    "            #一个batch=8个任务的数据集\n",
    "            x_spts.append(x_spt)  #(8，5,1,28,28)\n",
    "            y_spts.append(y_spt)\n",
    "            x_qrys.append(x_qry)  #（8，75，1，28，28）\n",
    "            y_qrys.append(y_qry)\n",
    "\n",
    "        # [b, setsz = n_way * k_spt, 1, 28, 28]\n",
    "        x_spts = np.array(x_spts).astype(np.float32).reshape(batch_size, setsz, 1, resize, resize)\n",
    "        y_spts = np.array(y_spts).astype(np.int).reshape(batch_size, setsz)\n",
    "        # [b, qrysz = n_way * k_query, 1, 28, 28]\n",
    "        x_qrys = np.array(x_qrys).astype(np.float32).reshape(batch_size, querysz, 1, resize, resize)\n",
    "        y_qrys = np.array(y_qrys).astype(np.int).reshape(batch_size, querysz)\n",
    "\n",
    "        data_cache.append([x_spts, y_spts, x_qrys, y_qrys])\n",
    "\n",
    "    return data_cache\n",
    "\n",
    "#元网络的数据集\n",
    "datasets_cache = {\"train\": load_data_cache(x_train),\n",
    "                  \"test\": load_data_cache(x_test)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#迭代地取出一个个batch的数据集\n",
    "def next(mode='train'):\n",
    "    \"\"\"\n",
    "    Gets next batch from the dataset with name.\n",
    "    :param mode: The name of the splitting (one of \"train\", \"val\", \"test\")\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # update cache if indexes is larger than len(data_cache)\n",
    "    if indexes[mode] >= len(datasets_cache[mode]):\n",
    "        indexes[mode] = 0\n",
    "        datasets_cache[mode] = load_data_cache(datasets[mode])\n",
    "\n",
    "    next_batch = datasets_cache[mode][indexes[mode]]\n",
    "    indexes[mode] += 1\n",
    "\n",
    "    return next_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "[0.2   0.385 0.635 0.685 0.685 0.71 ]\n",
      "[tensor(1.6094, device='cuda:0') tensor(1.4572, device='cuda:0')\n",
      " tensor(1.3445, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(1.2418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(1.1759, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(1.1320, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2062 0.4165 0.633  0.642  0.6553 0.667 ]\n",
      "epoch: 20\n",
      "[0.2   0.685 0.7   0.755 0.76  0.765]\n",
      "[tensor(1.6097, device='cuda:0') tensor(1.4005, device='cuda:0')\n",
      " tensor(1.2315, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(1.0760, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.9827, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.9167, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1996 0.6626 0.7534 0.778  0.7915 0.791 ]\n",
      "epoch: 40\n",
      "[0.215 0.77  0.825 0.845 0.845 0.83 ]\n",
      "[tensor(1.6055, device='cuda:0') tensor(1.3127, device='cuda:0')\n",
      " tensor(1.0564, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.8642, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.7675, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.7030, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 60\n",
      "[0.19  0.755 0.845 0.89  0.87  0.855]\n",
      "[tensor(1.6062, device='cuda:0') tensor(1.2851, device='cuda:0')\n",
      " tensor(1.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.8255, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.7145, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.6923, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1992 0.7515 0.8457 0.857  0.863  0.856 ]\n",
      "epoch: 80\n",
      "[0.2   0.785 0.885 0.92  0.915 0.905]\n",
      "[tensor(1.6118, device='cuda:0') tensor(1.1911, device='cuda:0')\n",
      " tensor(0.8832, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.6843, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5549, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5196, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2009 0.8315 0.8926 0.906  0.906  0.9062]\n",
      "epoch: 100\n",
      "[0.23  0.84  0.89  0.885 0.88  0.89 ]\n",
      "[tensor(1.6193, device='cuda:0') tensor(1.2420, device='cuda:0')\n",
      " tensor(0.9125, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.7204, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.6281, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5386, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 120\n",
      "[0.18  0.835 0.915 0.935 0.93  0.945]\n",
      "[tensor(1.6074, device='cuda:0') tensor(1.1762, device='cuda:0')\n",
      " tensor(0.8550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.6560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5320, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4597, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.201  0.827  0.9053 0.917  0.9194 0.918 ]\n",
      "epoch: 140\n",
      "[0.185 0.805 0.91  0.89  0.935 0.92 ]\n",
      "[tensor(1.6194, device='cuda:0') tensor(1.0876, device='cuda:0')\n",
      " tensor(0.7828, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.6439, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5217, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4533, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1986 0.8433 0.912  0.92   0.923  0.922 ]\n",
      "epoch: 160\n",
      "[0.175 0.795 0.85  0.875 0.89  0.89 ]\n",
      "[tensor(1.6142, device='cuda:0') tensor(1.1196, device='cuda:0')\n",
      " tensor(0.8236, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.6643, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5027, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 180\n",
      "[0.145 0.845 0.935 0.925 0.91  0.935]\n",
      "[tensor(1.6257, device='cuda:0') tensor(1.0380, device='cuda:0')\n",
      " tensor(0.7384, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5739, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4811, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3976, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1926 0.8496 0.9165 0.925  0.93   0.93  ]\n",
      "epoch: 200\n",
      "[0.165 0.805 0.915 0.95  0.94  0.93 ]\n",
      "[tensor(1.6480, device='cuda:0') tensor(1.0654, device='cuda:0')\n",
      " tensor(0.7656, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5874, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4780, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4556, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.198  0.8364 0.9253 0.93   0.931  0.933 ]\n",
      "epoch: 220\n",
      "[0.22  0.865 0.9   0.93  0.94  0.96 ]\n",
      "[tensor(1.6309, device='cuda:0') tensor(1.0373, device='cuda:0')\n",
      " tensor(0.7466, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5541, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4510, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3850, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 240\n",
      "[0.16  0.795 0.905 0.925 0.91  0.93 ]\n",
      "[tensor(1.6619, device='cuda:0') tensor(0.9915, device='cuda:0')\n",
      " tensor(0.7155, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4616, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4002, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1973 0.844  0.921  0.93   0.9326 0.933 ]\n",
      "epoch: 260\n",
      "[0.155 0.795 0.9   0.895 0.925 0.925]\n",
      "[tensor(1.6522, device='cuda:0') tensor(0.9871, device='cuda:0')\n",
      " tensor(0.6983, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5550, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4702, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4165, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2017 0.8584 0.9307 0.936  0.939  0.94  ]\n",
      "epoch: 280\n",
      "[0.21  0.84  0.93  0.935 0.94  0.935]\n",
      "[tensor(1.6808, device='cuda:0') tensor(0.9712, device='cuda:0')\n",
      " tensor(0.6943, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5499, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4845, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4093, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 300\n",
      "[0.215 0.795 0.87  0.87  0.89  0.9  ]\n",
      "[tensor(1.6157, device='cuda:0') tensor(0.9362, device='cuda:0')\n",
      " tensor(0.6927, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5644, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4904, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4032, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2072 0.8696 0.9287 0.939  0.9414 0.942 ]\n",
      "epoch: 320\n",
      "[0.17  0.925 0.97  0.93  0.965 0.965]\n",
      "[tensor(1.6733, device='cuda:0') tensor(0.8209, device='cuda:0')\n",
      " tensor(0.5286, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4279, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3108, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.2715, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1968 0.86   0.9233 0.9316 0.937  0.9385]\n",
      "epoch: 340\n",
      "[0.155 0.885 0.93  0.975 0.975 0.98 ]\n",
      "[tensor(1.6628, device='cuda:0') tensor(0.8156, device='cuda:0')\n",
      " tensor(0.5560, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3919, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3121, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.2570, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 360\n",
      "[0.125 0.87  0.91  0.935 0.94  0.89 ]\n",
      "[tensor(1.6723, device='cuda:0') tensor(0.8018, device='cuda:0')\n",
      " tensor(0.6071, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4566, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3778, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4105, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1989 0.8496 0.9155 0.9224 0.927  0.9326]\n",
      "epoch: 380\n",
      "[0.265 0.885 0.945 0.925 0.95  0.95 ]\n",
      "[tensor(1.6634, device='cuda:0') tensor(0.8180, device='cuda:0')\n",
      " tensor(0.5821, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4758, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3927, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3107, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2024 0.833  0.9023 0.9126 0.924  0.928 ]\n",
      "epoch: 400\n",
      "[0.135 0.84  0.935 0.925 0.94  0.94 ]\n",
      "[tensor(1.7681, device='cuda:0') tensor(0.8273, device='cuda:0')\n",
      " tensor(0.5857, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4632, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3791, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3396, device='cuda:0', grad_fn=<DivBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 420\n",
      "[0.2   0.85  0.82  0.915 0.92  0.94 ]\n",
      "[tensor(1.6726, device='cuda:0') tensor(0.8253, device='cuda:0')\n",
      " tensor(0.6787, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4796, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4255, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3455, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2035 0.824  0.899  0.9146 0.924  0.9277]\n",
      "epoch: 440\n",
      "[0.2   0.805 0.925 0.945 0.925 0.925]\n",
      "[tensor(1.7039, device='cuda:0') tensor(0.7979, device='cuda:0')\n",
      " tensor(0.5753, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4113, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3765, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3307, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2012 0.8164 0.892  0.9136 0.9243 0.9277]\n",
      "epoch: 460\n",
      "[0.235 0.845 0.91  0.93  0.945 0.93 ]\n",
      "[tensor(1.6951, device='cuda:0') tensor(0.7566, device='cuda:0')\n",
      " tensor(0.5103, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4079, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3068, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 480\n",
      "[0.195 0.755 0.89  0.88  0.91  0.895]\n",
      "[tensor(1.7555, device='cuda:0') tensor(0.8295, device='cuda:0')\n",
      " tensor(0.6097, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5042, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4276, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3896, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.199  0.794  0.8867 0.9077 0.9224 0.928 ]\n",
      "epoch: 500\n",
      "[0.155 0.76  0.92  0.925 0.955 0.93 ]\n",
      "[tensor(1.8046, device='cuda:0') tensor(0.8172, device='cuda:0')\n",
      " tensor(0.5336, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3896, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3567, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3321, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.199  0.7627 0.867  0.895  0.91   0.921 ]\n",
      "epoch: 520\n",
      "[0.22  0.81  0.88  0.895 0.92  0.9  ]\n",
      "[tensor(1.7471, device='cuda:0') tensor(0.8043, device='cuda:0')\n",
      " tensor(0.5592, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4797, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4040, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3702, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 540\n",
      "[0.16  0.71  0.845 0.865 0.79  0.88 ]\n",
      "[tensor(1.9530, device='cuda:0') tensor(0.8229, device='cuda:0')\n",
      " tensor(0.5885, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4971, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5761, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4858, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.194  0.77   0.868  0.8975 0.912  0.922 ]\n",
      "epoch: 560\n",
      "[0.255 0.735 0.855 0.87  0.945 0.905]\n",
      "[tensor(1.7430, device='cuda:0') tensor(0.8430, device='cuda:0')\n",
      " tensor(0.6254, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4774, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3571, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3494, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2014 0.742  0.8496 0.8887 0.905  0.916 ]\n",
      "epoch: 580\n",
      "[0.27  0.79  0.86  0.845 0.855 0.9  ]\n",
      "[tensor(1.6956, device='cuda:0') tensor(0.7367, device='cuda:0')\n",
      " tensor(0.5660, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4985, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4398, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3775, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 600\n",
      "[0.22  0.8   0.865 0.93  0.945 0.935]\n",
      "[tensor(1.7750, device='cuda:0') tensor(0.7884, device='cuda:0')\n",
      " tensor(0.5680, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4418, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3164, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1965 0.752  0.8535 0.8867 0.905  0.9155]\n",
      "epoch: 620\n",
      "[0.235 0.745 0.85  0.89  0.9   0.91 ]\n",
      "[tensor(1.9192, device='cuda:0') tensor(0.8442, device='cuda:0')\n",
      " tensor(0.5741, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4165, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3608, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3135, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2006 0.761  0.856  0.894  0.913  0.9214]\n",
      "epoch: 640\n",
      "[0.195 0.74  0.89  0.86  0.97  0.955]\n",
      "[tensor(1.9065, device='cuda:0') tensor(0.8160, device='cuda:0')\n",
      " tensor(0.5178, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4520, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.2963, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.2503, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 660\n",
      "[0.29  0.735 0.83  0.86  0.96  0.955]\n",
      "[tensor(1.9323, device='cuda:0') tensor(0.8411, device='cuda:0')\n",
      " tensor(0.6106, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4670, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3341, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.2933, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1963 0.7544 0.849  0.889  0.906  0.917 ]\n",
      "epoch: 680\n",
      "[0.27  0.685 0.81  0.84  0.885 0.875]\n",
      "[tensor(1.8441, device='cuda:0') tensor(0.8297, device='cuda:0')\n",
      " tensor(0.6171, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5026, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4182, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4042, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2024 0.7334 0.8384 0.8833 0.9033 0.916 ]\n",
      "epoch: 700\n",
      "[0.245 0.71  0.825 0.82  0.88  0.915]\n",
      "[tensor(1.8891, device='cuda:0') tensor(0.8317, device='cuda:0')\n",
      " tensor(0.6282, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5361, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4599, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3658, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 720\n",
      "[0.195 0.585 0.845 0.83  0.875 0.875]\n",
      "[tensor(2.0999, device='cuda:0') tensor(0.9794, device='cuda:0')\n",
      " tensor(0.5685, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5282, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4491, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4229, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1973 0.732  0.8306 0.8735 0.9014 0.9136]\n",
      "epoch: 740\n",
      "[0.18  0.71  0.855 0.89  0.905 0.935]\n",
      "[tensor(2.2279, device='cuda:0') tensor(0.7876, device='cuda:0')\n",
      " tensor(0.5494, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4306, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3722, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3170, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1996 0.7505 0.8525 0.8975 0.918  0.929 ]\n",
      "epoch: 760\n",
      "[0.24  0.705 0.805 0.91  0.91  0.92 ]\n",
      "[tensor(1.9031, device='cuda:0') tensor(0.7422, device='cuda:0')\n",
      " tensor(0.5807, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4173, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3718, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3188, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 780\n",
      "[0.215 0.73  0.835 0.895 0.915 0.92 ]\n",
      "[tensor(2.1107, device='cuda:0') tensor(0.7404, device='cuda:0')\n",
      " tensor(0.5300, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4039, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3263, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3032, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.195  0.727  0.845  0.8857 0.9067 0.9204]\n"
     ]
    }
   ],
   "source": [
    "#=======================================模型\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from copy import deepcopy, copy\n",
    "\n",
    "#构建基学习器,学习一个任务\n",
    "class BaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.vars = nn.ParameterList()  ## 包含了所有需要被优化的tensor参数w和b\n",
    "        self.vars_bn = nn.ParameterList()  ##bn层参数\n",
    "\n",
    "        # 第1个conv2d=================================\n",
    "        # in_channels = 1, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 1, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        # 第1个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        self.vars_bn.extend([running_mean, running_var])   #标准化？\n",
    "\n",
    "        # 第2个conv2d====================================\n",
    "        # in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        # 第2个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    "\n",
    "        # 第3个conv2d====================================\n",
    "        # in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        # 第3个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    "\n",
    "        # 第4个conv2d======================================\n",
    "        # in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        # 第4个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    "\n",
    "        ##linear===========================================\n",
    "        weight = nn.Parameter(torch.ones([5, 64]))\n",
    "        bias = nn.Parameter(torch.zeros(5))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "    #         self.conv = nn.Sequential(\n",
    "    #             nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2),\n",
    "    #             nn.BatchNorm2d(64),\n",
    "    #             nn.ReLU(),\n",
    "    #             nn.MaxPool2d(2),\n",
    "\n",
    "    #             nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2),\n",
    "    #             nn.BatchNorm2d(64),\n",
    "    #             nn.ReLU(),\n",
    "    #             nn.MaxPool2d(2),\n",
    "\n",
    "    #             nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2),\n",
    "    #             nn.BatchNorm2d(64),\n",
    "    #             nn.ReLU(),\n",
    "    #             nn.MaxPool2d(2),\n",
    "\n",
    "    #             nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2),\n",
    "    #             nn.BatchNorm2d(64),\n",
    "    #             nn.ReLU(),\n",
    "    #             nn.MaxPool2d(2),\n",
    "\n",
    "    #             FlattenLayer(),\n",
    "    #             nn.Linear(64,5)\n",
    "    #         )\n",
    "\n",
    "    def forward(self, x, params=None, bn_training=True):\n",
    "        '''\n",
    "        :bn_training: set False to not update\n",
    "        :return:\n",
    "        '''\n",
    "        if params is None:\n",
    "            params = self.vars\n",
    "\n",
    "        weight, bias = params[0], params[1]  # 第1个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride=2, padding=2)\n",
    "        weight, bias = params[2], params[3]  # 第1个BN层\n",
    "        running_mean, running_var = self.vars_bn[0], self.vars_bn[1]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight, bias=bias, training=bn_training)\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # 第1个MAX_POOL层\n",
    "        x = F.relu(x, inplace=[True])  # 第1个relu\n",
    "\n",
    "        weight, bias = params[4], params[5]  # 第2个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride=2, padding=2)\n",
    "        weight, bias = params[6], params[7]  # 第2个BN层\n",
    "        running_mean, running_var = self.vars_bn[2], self.vars_bn[3]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight, bias=bias, training=bn_training)\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # 第2个MAX_POOL层\n",
    "        x = F.relu(x, inplace=[True])  # 第2个relu\n",
    "\n",
    "        weight, bias = params[8], params[9]  # 第3个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride=2, padding=2)\n",
    "        weight, bias = params[10], params[11]  # 第3个BN层\n",
    "        running_mean, running_var = self.vars_bn[4], self.vars_bn[5]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight, bias=bias, training=bn_training)\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # 第3个MAX_POOL层\n",
    "        x = F.relu(x, inplace=[True])  # 第3个relu\n",
    "\n",
    "        weight, bias = params[12], params[13]  # 第4个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride=2, padding=2)\n",
    "        x = F.relu(x, inplace=[True])  # 第4个relu\n",
    "        weight, bias = params[14], params[15]  # 第4个BN层\n",
    "        running_mean, running_var = self.vars_bn[6], self.vars_bn[7]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight, bias=bias, training=bn_training)\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # 第4个MAX_POOL层\n",
    "\n",
    "        x = x.view(x.size(0), -1)  ## flatten\n",
    "        weight, bias = params[16], params[17]  # linear\n",
    "        x = F.linear(x, weight, bias)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.vars\n",
    "\n",
    "#构建元学习器，学习一个batch任务（8个任务为一个batch）\n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.update_step = 5  ## 基学习器训练更新5次\n",
    "        self.update_step_test = 5\n",
    "        self.net = BaseNet().cuda()\n",
    "        self.meta_lr = 2e-4  #元学习器的学习率\n",
    "        self.base_lr = 4 * 1e-2  #基学习器的学习率\n",
    "        self.inner_lr = 0.4\n",
    "        self.outer_lr = 1e-2\n",
    "        self.meta_optim = torch.optim.Adam(self.net.parameters(), lr=self.meta_lr)  #元学习器的优化器\n",
    "\n",
    "    def forward(self, x_spt, y_spt, x_qry, y_qry):  #输入一个batch的数据=8个任务的数据，每执行一次这个函数，元学习器就更新一次\n",
    "        x_spt, y_spt, x_qry, y_qry=x_spt.float(),y_spt.long(),x_qry.float(),y_qry.long()\n",
    "\n",
    "        #x_spt, y_spt, x_qry, y_qry=x_spt,torch.LongTensor(y_spt),torch.LongTensor(x_qry),torch.LongTensor(y_qry)\n",
    "        # 初始化\n",
    "        # 8         5    1    28  28\n",
    "        task_num, ways, shots, h, w = x_spt.size()\n",
    "        #      8，75，1，28，28\n",
    "        query_size = x_qry.size(1)  # 75 = 15 * 5\n",
    "        loss_list_qry = [0 for _ in range(self.update_step + 1)]   #[0,0,0,0,0,0]  #保存一个任务内，基学习器的初始损失加后面5次更新的损失\n",
    "        correct_list = [0 for _ in range(self.update_step + 1)]   #[0,0,0,0,0,0]  #保存一个任务内，基学习器的初始准确数加后面5次更新的准确数\n",
    "        #遍历一个batch内的每个任务，对每个任务训练更新基学习器5次\n",
    "        for i in range(task_num):\n",
    "            ## 一个任务上基学习器的第0步更新\n",
    "            y_hat = self.net(x_spt[i], params=None, bn_training=True)  # (ways * shots, ways) 用支持集，基学习器学习某一个任务\n",
    "\n",
    "            loss = F.cross_entropy(y_hat, y_spt[i]) #在支持集上，计算基学习器在某个任务的损失\n",
    "            grad = torch.autograd.grad(loss, self.net.parameters()) #计算该任务的梯度\n",
    "            tuples = zip(grad, self.net.parameters())  ## 将梯度和参数\\theta一一对应起来\n",
    "            # fast_weights这一步相当于求了一个\\theta - \\alpha*\\nabla(L)\n",
    "            fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], tuples))  #更新一次后的基学习器参数\n",
    "            # 在query集上测试，计算准确率\n",
    "            # 这一步使用更新前的数据\n",
    "            with torch.no_grad():\n",
    "                y_hat = self.net(x_qry[i], self.net.parameters(), bn_training=True) #使用前一步的基学习器计算查询集上的损失和准确数\n",
    "                loss_qry = F.cross_entropy(y_hat, y_qry[i])\n",
    "                loss_list_qry[0] += loss_qry\n",
    "                pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "                correct = torch.eq(pred_qry, y_qry[i]).sum().item()\n",
    "                correct_list[0] += correct\n",
    "\n",
    "            # 使用更新后的数据在query集上测试。\n",
    "            with torch.no_grad():\n",
    "                y_hat = self.net(x_qry[i], fast_weights, bn_training=True)  #使用更新后的基学习器计算查询集上的损失和准确数\n",
    "                loss_qry = F.cross_entropy(y_hat, y_qry[i])\n",
    "                loss_list_qry[1] += loss_qry\n",
    "                pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "                correct = torch.eq(pred_qry, y_qry[i]).sum().item()\n",
    "                correct_list[1] += correct\n",
    "\n",
    "            for k in range(1, self.update_step):   #再进行4次支持集上训练后更新基学习器的参数，并记录查询集上的验证损失和准确数\n",
    "                y_hat = self.net(x_spt[i], params=fast_weights, bn_training=True)\n",
    "                loss = F.cross_entropy(y_hat, y_spt[i])\n",
    "                grad = torch.autograd.grad(loss, fast_weights)\n",
    "                tuples = zip(grad, fast_weights)\n",
    "                fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], tuples))\n",
    "\n",
    "                y_hat = self.net(x_qry[i], params=fast_weights, bn_training=True)\n",
    "                loss_qry = F.cross_entropy(y_hat, y_qry[i])\n",
    "                loss_list_qry[k + 1] += loss_qry\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)\n",
    "                    correct = torch.eq(pred_qry, y_qry[i]).sum().item()\n",
    "                    correct_list[k + 1] += correct\n",
    "        #         print('hello')\n",
    "\n",
    "        #元学习器参数更新\n",
    "        loss_qry = loss_list_qry[-1] / task_num  #计算最后一次更新基学习器（第五次更新）上查询集的平均每个任务的损失（表示这个batch的损失）===》作为外部元学习器的一次损失，更新一次元学习器的参数\n",
    "        self.meta_optim.zero_grad()  # 梯度清零\n",
    "        loss_qry.backward() #\n",
    "        self.meta_optim.step()\n",
    "\n",
    "        #计算当前这步的元学习器学到的基学习器对于一个任务1+（5次更新）的对应的查询集平均准确率和损失\n",
    "        accs = np.array(correct_list) / (query_size * task_num)\n",
    "        loss = np.array(loss_list_qry) / (task_num)\n",
    "        return accs, loss  #返回每一步（基学习器在每个任务上更新共六步）的平均损失和准确数\n",
    "\n",
    "    #对MAML训练得到的基学习器测试准确率\n",
    "    def finetunning(self, x_spt, y_spt, x_qry, y_qry):  #输入一个任务的数据\n",
    "        assert len(x_spt.shape) == 4\n",
    "        x_spt, y_spt, x_qry, y_qry = x_spt.float(), y_spt.long(), x_qry.float(), y_qry.long()\n",
    "\n",
    "        query_size = x_qry.size(0)  #75\n",
    "        correct_list = [0 for _ in range(self.update_step_test + 1)]   #[0,0,0,0,0,0]\n",
    "\n",
    "        new_net = deepcopy(self.net)\n",
    "        y_hat = new_net(x_spt)\n",
    "        loss = F.cross_entropy(y_hat, y_spt)\n",
    "        grad = torch.autograd.grad(loss, new_net.parameters())\n",
    "        fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], zip(grad, new_net.parameters())))\n",
    "\n",
    "        # 在query集上测试，计算准确率\n",
    "        # 这一步使用更新前的基学习器参数\n",
    "        with torch.no_grad():\n",
    "            y_hat = new_net(x_qry, params=new_net.parameters(), bn_training=True)\n",
    "            pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "            correct = torch.eq(pred_qry, y_qry).sum().item()\n",
    "            correct_list[0] += correct\n",
    "\n",
    "        # 使用更新后的基学习器参数。\n",
    "        with torch.no_grad():\n",
    "            y_hat = new_net(x_qry, params=fast_weights, bn_training=True)\n",
    "            pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "            correct = torch.eq(pred_qry, y_qry).sum().item()\n",
    "            correct_list[1] += correct\n",
    "\n",
    "        for k in range(1, self.update_step_test):  #五步更新基学习器（这里是剩下四步）\n",
    "            y_hat = new_net(x_spt, params=fast_weights, bn_training=True)\n",
    "            loss = F.cross_entropy(y_hat, y_spt)\n",
    "            grad = torch.autograd.grad(loss, fast_weights)\n",
    "            fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "            y_hat = new_net(x_qry, fast_weights, bn_training=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_qry, y_qry).sum().item()\n",
    "                correct_list[k + 1] += correct\n",
    "\n",
    "        del new_net\n",
    "        accs = np.array(correct_list) / query_size  #返回一个任务上基学习器的平均准确率\n",
    "        return accs\n",
    "\n",
    "#模型训练\n",
    "import time\n",
    "device = torch.device('cuda')\n",
    "\n",
    "meta = MetaLearner().to(device)  #初始化元学习器\n",
    "\n",
    "epochs =800 # 60000\n",
    "for step in range(epochs):\n",
    "    start = time.time()\n",
    "    x_spt, y_spt, x_qry, y_qry = next('train')  #取出一个batch的数据（包含8个任务）\n",
    "    x_spt, y_spt, x_qry, y_qry = torch.from_numpy(x_spt).to(device), \\\n",
    "                                 torch.from_numpy(y_spt).to(device), \\\n",
    "                                 torch.from_numpy(x_qry).to(device), \\\n",
    "                                 torch.from_numpy(y_qry).to(device)\n",
    "    accs, loss = meta(x_spt, y_spt, x_qry, y_qry)  #更新一次元学习器，返回这一次在八个任务上基学习器每一步更新得到的的损失和准确率\n",
    "    end = time.time()\n",
    "\n",
    "    if step % 20 == 0:  #每更新100次元学习器，输出一次当前的基学习器准确率和损失情况\n",
    "        print(\"epoch:\", step)\n",
    "        print(accs)\n",
    "        print(loss)\n",
    "\n",
    "    if step % 30 == 0:  #每更新1000次元学习器，从测试集分别取出一个batch数据，从中依此取出一个任务的数据测试当前学到的基学习器\n",
    "        accs = []\n",
    "        for _ in range(1000 // task_num):\n",
    "            # db_train.next('test')\n",
    "            x_spt, y_spt, x_qry, y_qry = next('test')\n",
    "            x_spt, y_spt, x_qry, y_qry = torch.from_numpy(x_spt).to(device), \\\n",
    "                                         torch.from_numpy(y_spt).to(device), \\\n",
    "                                         torch.from_numpy(x_qry).to(device), \\\n",
    "                                         torch.from_numpy(y_qry).to(device)\n",
    "\n",
    "            for x_spt_one, y_spt_one, x_qry_one, y_qry_one in zip(x_spt, y_spt, x_qry, y_qry):\n",
    "                test_acc = meta.finetunning(x_spt_one, y_spt_one, x_qry_one, y_qry_one)\n",
    "                accs.append(test_acc)\n",
    "        print('在mean process之前：', np.array(accs).shape)\n",
    "        accs = np.array(accs).mean(axis=0).astype(np.float16)\n",
    "        print('测试集准确率:', accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyenv] *",
   "language": "python",
   "name": "conda-env-pyenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
