{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 数据预处理\n",
    "root_dir = 'D:\\\\数据库\\\\小样本数据库\\\\data\\\\omniglot2\\\\raw'\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 生成数据集的x，y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.把每个样本存为三元组格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0709_01.png', 'Alphabet_of_the_Magi/character01', 'D:\\\\数据库\\\\小样本数据库\\\\data\\\\omniglot2\\\\raw\\\\images_background\\\\Alphabet_of_the_Magi\\\\character01')\n"
     ]
    }
   ],
   "source": [
    "#返回一个img_items列表，包含所有图片样本，每个样本都是一个三元组（图片名词，语言/字母类别，图片样本所在的目录）\n",
    "def find_classes(root_dir):\n",
    "    img_items = []\n",
    "    for (root, dirs, files) in os.walk(root_dir):\n",
    "        for file in files:  #遍历某种语言某类字母的20张图片\n",
    "            if (file.endswith(\"png\")):\n",
    "                r = root.split('\\\\')\n",
    "                img_items.append((file, r[-2] + \"/\" + r[-1], root))\n",
    "    #print(\"== Found %d items \" % len(img_items))   #32460个样本\n",
    "    return img_items\n",
    "\n",
    "img_items = find_classes(root_dir)\n",
    "print(img_items[0])\n",
    "# print(img_items[0])  \n",
    "# ('0709_01.png', \n",
    "#  'Alphabet_of_the_Magi/character01', \n",
    "#  'D:/数据库/小样本数据库/data/omniglot2/raw\\\\images_background\\\\Alphabet_of_the_Magi\\\\character01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.构造类别映射字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 构建一个词典{class:idx}  把所有类（具体到所有语言的所有字母类别）构成一个映射，索引按顺序排列\n",
    "def index_classes(items):\n",
    "    class_idx = {}\n",
    "    count = 0\n",
    "    for item in items:\n",
    "        if item[1] not in class_idx:\n",
    "            class_idx[item[1]] = count\n",
    "            count += 1\n",
    "    #print('== Found {} classes'.format(len(class_idx)))   #1623类\n",
    "    return class_idx\n",
    "\n",
    "class_idx = index_classes(img_items)\n",
    "#print(class_idx)  #{'Alphabet_of_the_Magi/character01': 0, 'Alphabet_of_the_Magi/character02': 1;,,,,1622}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.构造字典，按类别存放样本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dict()  #把所有样本生成一个字典按类别保存，(类别索引：[图片1，图片2，，，图片20])\n",
    "for imgname, classes, dirs in img_items:\n",
    "    img = '{}/{}'.format(dirs, imgname)  #某张图片的地址\n",
    "    label = class_idx[classes]  #某张图片的类别对应的索引\n",
    "    transform = transforms.Compose([lambda img: Image.open(img).convert('L'),  #转成灰度图\n",
    "                                    lambda img: img.resize((28, 28)),  #统一大小\n",
    "                                    lambda img: np.reshape(img, (28, 28, 1)),\n",
    "                                    lambda img: np.transpose(img, [2, 0, 1]),   #通道在前\n",
    "                                    lambda img: img / 255.  #归一化\n",
    "                                    ])\n",
    "    img = transform(img)  #图像处理\n",
    "    if label in temp.keys():\n",
    "        temp[label].append(img)\n",
    "    else:\n",
    "        temp[label] = [img]\n",
    "        \n",
    "#print(len(temp))  #1623"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.把字典转换成数组，按类划分小数组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#存放所有样本图片[[img1,,,,20张图片],[],[],[],,,,,1623个[]]\n",
    "img_list = []   \n",
    "for label, imgs in temp.items():\n",
    "    img_list.append(np.array(imgs))\n",
    "img_list = np.array(img_list).astype(np.float)\n",
    "#print('data shape:{}'.format(img_list.shape))  # (1623, 20, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.分割训练集/验证集/测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = img_list.shape[0]  #训练集+测试集一共有1623类\n",
    "x_train = img_list[:1100]  #前1100类作为训练集\n",
    "x_val = img_list[1100:1300]  #后200类作为验证集\n",
    "x_test=img_list[1300:]  #后323类作为测试集\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.把数据集分割成x和y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_x_y(subset):\n",
    "    imgs=[]  #数据集所有图片\n",
    "    labels=[] #imgs对应的标签\n",
    "    for i in range(len(subset)):\n",
    "        for j in range(20):\n",
    "            img=subset[i][j]\n",
    "            imgs.append(img)\n",
    "            labels.append(i)\n",
    "    return imgs,labels\n",
    "\n",
    "train_imgs,train_labels=split_x_y(x_train)\n",
    "val_imgs,val_labels=split_x_y(x_val)\n",
    "test_imgs,test_labels=split_x_y(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义dataset类和dataloader类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "\n",
    "class PrototypicalDataset(data.Dataset):\n",
    "    def __init__(self,imgs,labels):\n",
    "        self.imgs=imgs\n",
    "        self.labels=labels\n",
    "    def __getitem__(self, idx):\n",
    "        return self.imgs[idx], self.labels[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我改了__init__部分\n",
    "# class PrototypicalBatchSampler(object):\n",
    "#     def __init__(self, labels, classes_per_it, num_samples, iterations):\n",
    "#         super(PrototypicalBatchSampler, self).__init__()\n",
    "#         self.labels = labels\n",
    "#         self.classes_per_it = classes_per_it\n",
    "#         self.sample_per_class = num_samples  #s+q\n",
    "#         self.iterations = iterations  #一个batch包含多少个任务，iterations=episode（s+q）\n",
    "\n",
    "#         #数据集一共有多少个类别，每个类别各有多少个样本\n",
    "#         self.classes, self.counts = np.unique(self.labels, return_counts=True)\n",
    "#         self.classes = torch.LongTensor(self.classes)\n",
    "\n",
    "#         #数据集一共有多少个样本\n",
    "#         self.idxs = range(len(self.labels)) #[0~2399]\n",
    "#         #（类别数，每个类别的样本数）矩阵，存放所有样本，值为某类某个样本在（1~数据集总样本）中排第几个\n",
    "#         self.indexes = np.empty((len(self.classes), max(self.counts)), dtype=int) * np.nan\n",
    "#         self.indexes = torch.Tensor(self.indexes)\n",
    "       \n",
    "#         for idx, label in enumerate(self.labels):\n",
    "#             for i in range(20):\n",
    "#                 self.indexes[label,i]=idx\n",
    "    \n",
    "#     #产生一个batch样本的索引,其中包含iterations个任务\n",
    "#     def __iter__(self):\n",
    "#         cpi = self.classes_per_it  #每个任务随机选出多少类样本\n",
    "#         spc = self.sample_per_class  #每类样本需要多少个=s+q\n",
    "       \n",
    "#         for it in range(self.iterations):  #一个batch包含self.iterations个任务,iterations相当于eposides\n",
    "#             eposide_size = spc * cpi  #一个任务需要的数据量（支持集+查询集）\n",
    "#             eposide = torch.LongTensor(eposide_size)\n",
    "#             c_idxs = torch.randperm(len(self.classes))[:cpi]  #从数据集1200个类别中随机选取cpi个类别\n",
    "\n",
    "#             for i, c in enumerate(self.classes[c_idxs]):   #从被选出的每个类别中随机选取spc个样本\n",
    "#                 s = slice(i * spc, (i + 1) * spc) #每个类被选出的所有样本在新生成eposide中的存放位置\n",
    "\n",
    "#                 label_idx=c\n",
    "#                 sample_idxs = torch.randperm(20)[:spc]  #从某一类20个样本中随机选择spc个样本\n",
    "#                 eposide[s] = self.indexes[label_idx][sample_idxs]\n",
    "#             eposide = eposide[torch.randperm(len(eposide))]  #打乱eposide内部的数据顺序\n",
    "#             yield eposide  #next(iter(loader))时，iterations次迭代产生eposide_size个数据作为一次更新网络的数据\n",
    "#     def __len__(self):\n",
    "#         return self.iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrototypicalBatchSampler(object):\n",
    "    def __init__(self, labels, classes_per_it, num_samples, iterations):\n",
    "        super(PrototypicalBatchSampler, self).__init__()\n",
    "        self.labels = labels\n",
    "        self.classes_per_it = classes_per_it\n",
    "        self.sample_per_class = num_samples  #s+q\n",
    "        self.iterations = iterations\n",
    "\n",
    "        self.classes, self.counts = np.unique(self.labels, return_counts=True)\n",
    "        #print(self.classes, self.counts )   #[0~1199]   [20,,,,,,20]\n",
    "        self.classes = torch.LongTensor(self.classes)\n",
    "\n",
    "        self.idxs = range(len(self.labels)) #[0~2399]\n",
    "        self.indexes = np.empty((len(self.classes), max(self.counts)), dtype=int) * np.nan\n",
    "        self.indexes = torch.Tensor(self.indexes)\n",
    "        self.numel_per_class = torch.zeros_like(self.classes)\n",
    "        for idx, label in enumerate(self.labels):\n",
    "            label_idx = np.argwhere(self.classes == label).item()\n",
    "            self.indexes[label_idx, np.where(np.isnan(self.indexes[label_idx]))[0][0]] = idx\n",
    "            self.numel_per_class[label_idx] += 1\n",
    "        #print(self.indexes[2][15])  #  55     self.indexes ([1200, 20])   表示第n类的第k张图片在数据集中排第几个样本\n",
    "        #print(self.numel_per_class[0])  #   20    self.numel_per_class([1200])  \n",
    "    \n",
    "    #产生一个batch样本的索引\n",
    "    def __iter__(self):\n",
    "        spc = self.sample_per_class\n",
    "        cpi = self.classes_per_it\n",
    "\n",
    "        for it in range(self.iterations):  #一个batch包含self.iterations个任务,iterations相当于eposides\n",
    "            batch_size = spc * cpi  #一个任务需要的数据量（支持集+查询集）\n",
    "            batch = torch.LongTensor(batch_size)\n",
    "            c_idxs = torch.randperm(len(self.classes))[:cpi]  #从数据集1200个类别中随机选取cpi个类别\n",
    "            #print(c_idxs)  #([ 335,   56, 1139,  991,  137])\n",
    "            for i, c in enumerate(self.classes[c_idxs]):   #从被选出的每个类别中随机选取spc个样本\n",
    "                s = slice(i * spc, (i + 1) * spc) #每个类被选出的所有样本在新生成batch中的存放位置\n",
    "                #print('c=%d',c)  #c分别为335,   56, 1139,  991,  137\n",
    "                #print('i=%d',i) #0~4\n",
    "                #label_idx = torch.arange(len(self.classes)).long()[self.classes == c].item()\n",
    "                label_idx=c\n",
    "                sample_idxs = torch.randperm(20)[:spc]  #从某一类20个样本中随机选择spc个样本\n",
    "                batch[s] = self.indexes[label_idx][sample_idxs]\n",
    "            batch = batch[torch.randperm(len(batch))]\n",
    "            yield batch\n",
    "    def __len__(self):\n",
    "        return self.iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataloader(imgs,labels,classes_per_it, num_samples, iterations):\n",
    "    dataset=PrototypicalDataset(imgs,labels)\n",
    "    sampler=PrototypicalBatchSampler(labels,classes_per_it, num_samples, iterations)\n",
    "    return torch.utils.data.DataLoader(dataset, batch_sampler=sampler)\n",
    "\n",
    "train_loader=dataloader(train_imgs,train_labels,5,15,28)\n",
    "val_loader=dataloader(val_imgs,val_labels,5,15,28)\n",
    "test_loader=dataloader(test_imgs,test_labels,5,15,28)\n",
    "#next(iter(train_loader))   #[75个图片矩阵，75个标签]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造原型网络模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ProtoNet(\n",
      "  (encoder): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (1): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (2): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (3): Sequential(\n",
      "      (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def conv_block(in_channels, out_channels):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "\n",
    "\n",
    "class ProtoNet(nn.Module):\n",
    "    def __init__(self, x_dim=1, hid_dim=64, z_dim=64):\n",
    "        super(ProtoNet, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            conv_block(x_dim, hid_dim),\n",
    "            conv_block(hid_dim, hid_dim),\n",
    "            conv_block(hid_dim, hid_dim),\n",
    "            conv_block(hid_dim, z_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x=x.type(torch.cuda.FloatTensor)\n",
    "        x = self.encoder(x)\n",
    "        return x.view(x.size(0), -1)\n",
    "    \n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "model = ProtoNet().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim=torch.optim.Adam(params=model.parameters(),lr=0.001)\n",
    "lr_scheduler=torch.optim.lr_scheduler.StepLR(optimizer=optim,gamma=0.5,step_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "#欧式距离\n",
    "def euclidean_dist(x, y):\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "    n = x.size(0)  #查询集的样本数=原型的数*n_query\n",
    "    m = y.size(0)  #原型的数\n",
    "    d = x.size(1)  #每个样本的嵌入网络输出维度=每个原型的嵌入维度\n",
    "    if d != y.size(1):\n",
    "        raise Exception\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)  #返回（n,m）\n",
    "\n",
    "def prototypical_loss(input, target, n_support):\n",
    "    target_cpu = target.to('cpu')   #75个样本的类别\n",
    "    input_cpu = input.to('cpu')  #75个样本全部网络嵌入后的输出结果\n",
    "\n",
    "    #返回类别c选出的支持集样本索引\n",
    "    def support_idxs(c):\n",
    "        return target_cpu.eq(c).nonzero()[:n_support].squeeze(1)  #从类别c的所有样本中取n_support个作为支持集\n",
    "    #返回类别c选出的查询集样本索引\n",
    "    def query_idxs(c):\n",
    "        return target_cpu.eq(c).nonzero()[n_support:].squeeze(1)\n",
    "    \n",
    "    classes = torch.unique(target_cpu)  #一共有哪些类别\n",
    "    n_classes = len(classes)  #一共有多少给类别\n",
    "    n_query = target_cpu.eq(classes[0].item()).sum().item() - n_support  #每个类别各有多少个样本组成查询集\n",
    "\n",
    "    support_idxs = list(map(support_idxs, classes))   #全部支持集样本的索引[[类别1]，,,[类别n_classes]]\n",
    "    prototypes = torch.stack([input_cpu[idx_list].mean(0) for idx_list in support_idxs])   #计算出支持集每个类别的原型\n",
    "\n",
    "    query_idxs = torch.stack(list(map(query_idxs, classes))).view(-1)\n",
    "    query_samples = input.to('cpu')[query_idxs]     #取出所有支持集样本的嵌入向量\n",
    "                             \n",
    "    dists = euclidean_dist(query_samples, prototypes)  #返回（查询集样本数，原型类别数）的距离矩阵\n",
    "\n",
    "    log_p_y = F.log_softmax(-dists, dim=1).view(n_classes, n_query, -1)\n",
    "\n",
    "    target_inds = torch.arange(0, n_classes)\n",
    "    target_inds = target_inds.view(n_classes, 1, 1)\n",
    "    target_inds = target_inds.expand(n_classes, n_query, 1).long()\n",
    "\n",
    "    loss_val = -log_p_y.gather(2, target_inds).squeeze().view(-1).mean()\n",
    "    _, y_hat = log_p_y.max(2)  #把log_p_y最小的原型对应的类别定为查询集样本的类别\n",
    "    acc_val = y_hat.eq(target_inds.squeeze()).float().mean()\n",
    "\n",
    "    return loss_val,  acc_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/28 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Epoch: 0 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [00:02<00:00, 11.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Train Loss: 0.48896934158567873, Avg Train Acc: 0.8299999982118607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████▉                                                                          | 3/28 [00:00<00:01, 22.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Val Loss: 1.0117226775203432, Avg Val Acc: 0.8378571357045855 (Best)\n",
      "=== Epoch: 1 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [00:01<00:00, 19.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Train Loss: 0.35199573722534944, Avg Train Acc: 0.8782142805201667\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████▉                                                                          | 3/28 [00:00<00:01, 20.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Val Loss: 0.6877503550744483, Avg Val Acc: 0.8585714218871934 (Best)\n",
      "=== Epoch: 2 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [00:01<00:00, 19.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Train Loss: 0.285249797383412, Avg Train Acc: 0.900952377489635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████▉                                                                          | 3/28 [00:00<00:01, 20.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Val Loss: 0.5660900171136573, Avg Val Acc: 0.8659523746797017 (Best)\n",
      "=== Epoch: 3 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [00:01<00:00, 19.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Train Loss: 0.20838089808356017, Avg Train Acc: 0.9269999969005585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|████████▉                                                                          | 3/28 [00:00<00:01, 20.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Val Loss: 0.4267918811738491, Avg Val Acc: 0.8853999948501587 (Best)\n",
      "=== Epoch: 4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 28/28 [00:01<00:00, 19.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Train Loss: 0.16582623889204115, Avg Train Acc: 0.9443999975919724\n",
      "Avg Val Loss: 0.3104475658386946, Avg Val Acc: 0.8907999962568283 (Best)\n",
      "最后的结果： best_acc=0, train_loss=0, train_acc=0, val_loss=0, val_acc=0\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "train_loss = [] #记录每个eposide，每个任务更新的结果\n",
    "train_acc = []\n",
    "\n",
    "val_loss = []\n",
    "val_acc = []\n",
    "\n",
    "best_acc = 0\n",
    "best_model_path='C:/Users/user/Desktop/proto_net/best_protonet/best_model.pth'\n",
    "last_model_path='C:/Users/user/Desktop/proto_net/best_protonet/last_model.pth'\n",
    "\n",
    "#遍历epoch训练\n",
    "for epoch in range(5):\n",
    "    print('=== Epoch: {} ==='.format(epoch))\n",
    "    #取一个batch的训练数据\n",
    "    tr_iter = iter(train_loader)\n",
    "    model.train()\n",
    "    for eposide in tqdm(tr_iter):  #遍历每个eposide，一共遍历iterations次，#len(next(iter(dataloader))) #[75个图片矩阵，75个标签]\n",
    "        optim.zero_grad()  #梯度清零\n",
    "        x, y = eposide  #x.shape=([75, 1, 28, 28])\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        model_output = model(x)  #前传\n",
    "        loss, acc = prototypical_loss(model_output, target=y,n_support=5)  #计算loss\n",
    " \n",
    "        loss.backward()  #反传\n",
    "        optim.step()  #参数更新\n",
    "        \n",
    "        train_loss.append(loss.item())   #加入这个batch的计算loss\n",
    "        train_acc.append(acc.item())  #加入这个batch的计算acc\n",
    "        \n",
    "    avg_loss = np.mean(train_loss[-100:])  #计算本次epoch的所有batch迭代的平均损失\n",
    "    avg_acc = np.mean(train_acc[-100:])   #计算本次epoch的所有batch迭代的平均准确率\n",
    "    print('Avg Train Loss: {}, Avg Train Acc: {}'.format(avg_loss, avg_acc))\n",
    "    lr_scheduler.step()   #更新学习率\n",
    "    \n",
    "    #每训练一个batch=iterations个任务后，验证一次模型\n",
    "    val_iter = iter(val_loader)\n",
    "    model.eval()\n",
    "    for batch in val_iter:\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        model_output = model(x)\n",
    "        loss, acc = prototypical_loss(model_output, target=y,n_support=5)\n",
    "        val_loss.append(loss.item())\n",
    "        val_acc.append(acc.item())\n",
    "\n",
    "    avg_loss = np.mean(val_loss[-100:])\n",
    "    avg_acc = np.mean(val_acc[-100:])\n",
    "\n",
    "    #如果当前模型的验证效果比先前好，则把当前模型记为最佳模型\n",
    "    postfix = ' (Best)' if avg_acc >= best_acc else ' (Best: {})'.format(best_acc)\n",
    "    print('Avg Val Loss: {}, Avg Val Acc: {}{}'.format(  #输出本次验证的平均损失和准确率\n",
    "            avg_loss, avg_acc, postfix))\n",
    "\n",
    "    #保存某个epoch内所有batch在验证集上得到的最佳模型、准确率\n",
    "    if avg_acc >= best_acc:\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        best_acc = avg_acc\n",
    "        best_state = model.state_dict()\n",
    "\n",
    "#print(len(train_loss))  #iterations\n",
    "#print(len(val_loss)) #iterations\n",
    "\n",
    "#保存最终所有epoch得到的最佳模型\n",
    "torch.save(model.state_dict(), last_model_path)\n",
    "print('最后的结果： best_acc=%d, train_loss=%d, train_acc=%d, val_loss=%d, val_acc=%d' % (best_acc, train_loss[-1], train_acc[-1], val_loss[-1], val_acc[-1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Acc: 0.9172142833471298\n"
     ]
    }
   ],
   "source": [
    "avg_acc = list()\n",
    "for epoch in range(10):\n",
    "    test_iter = iter(test_loader)\n",
    "    for batch in test_iter:\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        model_output = model(x)\n",
    "        _, acc = prototypical_loss(model_output, target=y,n_support=5)\n",
    "        avg_acc.append(acc.item())\n",
    "avg_acc = np.mean(avg_acc)\n",
    "print('Test Acc: {}'.format(avg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with best model..\n",
      "Test Acc: 0.9250714255230768\n"
     ]
    }
   ],
   "source": [
    "model=None\n",
    "#加载训练最好的模型参数\n",
    "model = ProtoNet().to(device)\n",
    "model.load_state_dict(best_state)\n",
    "print('Testing with best model..')\n",
    "#测试模型\n",
    "avg_acc = list()\n",
    "for epoch in range(10):\n",
    "    test_iter = iter(test_loader)\n",
    "    for batch in test_iter:\n",
    "        x, y = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        model_output = model(x)\n",
    "        _, acc = prototypical_loss(model_output, target=y,n_support=5)\n",
    "        avg_acc.append(acc.item())\n",
    "avg_acc = np.mean(avg_acc)\n",
    "print('Test Acc: {}'.format(avg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyenv] *",
   "language": "python",
   "name": "conda-env-pyenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
