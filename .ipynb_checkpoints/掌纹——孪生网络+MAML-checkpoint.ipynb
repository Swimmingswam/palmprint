{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import numpy.random as rng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(460, 5, 1, 30, 30)\n",
      "(460, 5)\n"
     ]
    }
   ],
   "source": [
    "labels=[]\n",
    "images=[]\n",
    "for i in range(460):\n",
    "    m=i+1\n",
    "    samples_num=os.listdir(os.path.join('D:/数据库/palmdata/iitd',str(m)))\n",
    "    imgs=[]\n",
    "    labs=[]\n",
    "    for k in range(5):  #len(samples_num)\n",
    "        j=k+1\n",
    "        tpath=os.path.join(r'D:/数据库/palmdata/iitd',str(m),str(j)+'.jpeg')     #路径(/home/ouc/river/test)+图片名（img_m）\n",
    "        fopen = Image.open(tpath)\n",
    "        transform=transforms.Compose([transforms.Resize((img_size, img_size)),  # 将图片缩放到指定大小（h,w）或者保持长宽比并缩放最短的边到int大小\n",
    "                                         transforms.Grayscale(),\n",
    "                                         transforms.ToTensor()]) \n",
    "        data=np.array(transform(fopen))#data就是预处理后，可以送入模型进行训练的数据了\n",
    "        imgs.append(data)\n",
    "        labs.append(m)\n",
    "    labels.append(labs)\n",
    "    images.append(imgs)\n",
    "\n",
    "labels=np.array(labels)\n",
    "images=np.array(images)\n",
    "print(images.shape) #(460,5)\n",
    "print(labels.shape)   #(460,5,1, 28, 28)\n",
    "\n",
    "# train_x=images[:2025]   #前360类数据作为训练集\n",
    "# train_y=labels[:2025]\n",
    "# val_x=images[2025:2600]  #后100类数据作为测试集\n",
    "# val_y=labels[2025:2600]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练集\n",
    "x_train=images[:360]\n",
    "y_train=labels[:360]\n",
    "#测试集\n",
    "x_test=images[360:]\n",
    "y_test=labels[360:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17  18\n",
      "  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36\n",
      "  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54\n",
      "  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72\n",
      "  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90\n",
      "  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108\n",
      " 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126\n",
      " 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144\n",
      " 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162\n",
      " 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180\n",
      " 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198\n",
      " 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216\n",
      " 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234\n",
      " 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252\n",
      " 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270\n",
      " 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288\n",
      " 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306\n",
      " 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324\n",
      " 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342\n",
      " 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 360 is out of bounds for axis 0 with size 360",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-72fe00c54f45>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtasks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m \u001b[0mmake_pairs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-43-72fe00c54f45>\u001b[0m in \u001b[0;36mmake_pairs\u001b[1;34m(images, labels, base_batch_size, num_tasks)\u001b[0m\n\u001b[0;32m     17\u001b[0m                         \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mex1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mex2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrng\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mimg1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mselected_cls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mex1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[0mimg2\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcls_2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mex2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mflag\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgenuine_imposter_ratio\u001b[0m \u001b[1;31m#返回0/1标记\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 360 is out of bounds for axis 0 with size 360"
     ]
    }
   ],
   "source": [
    "#生成一个epoch的模型的训练集或测试集列表，[support_set_x, support_set_y, target_x, target_y]\n",
    "def make_pairs(images,labels,base_batch_size,num_tasks):\n",
    "    classes,counts = np.unique(labels, return_counts=True)\n",
    "    print(classes)\n",
    "    tasks=[]  #保存一个元batch的数据，长度是num_tasks\n",
    "    for _ in range(num_tasks):  #生成8个任务\n",
    "        task=[] #保存一个任务的数据，长度是base_batch_size\n",
    "        for sample in range(base_batch_size):\n",
    "            selected_cls = random.choice(classes)   #选择第一个样本的类\n",
    "            genuine_imposter_ratio=random.randint(0,1)\n",
    "            if genuine_imposter_ratio: #正例\n",
    "                cls_2=selected_cls\n",
    "            else:  #负例\n",
    "                while True:\n",
    "                    cls_2=random.choice(classes)\n",
    "                    if cls_2!=selected_cls:\n",
    "                        break\n",
    "            ex1, ex2 = rng.choice(5,replace=False,size=(2,)) \n",
    "            print()\n",
    "            img1=images[selected_cls][ex1]\n",
    "            img2=images[cls_2][ex2]\n",
    "            flag=genuine_imposter_ratio #返回0/1标记\n",
    "            task.append((img1,img2,flag))\n",
    "        task=np.array(task)\n",
    "        tasks.append(task)\n",
    "    tasks=np.array(tasks)\n",
    "    print(tasks.shape)  #(8, 16, 3)\n",
    "    return tasks   \n",
    "\n",
    "make_pairs(x_train,y_train,64,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#迭代地取出一个个batch的数据集\n",
    "def next(mode='train'):\n",
    "    \"\"\"\n",
    "    Gets next batch from the dataset with name.\n",
    "    :param mode: The name of the splitting (one of \"train\", \"val\", \"test\")\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # update cache if indexes is larger than len(data_cache)\n",
    "    if indexes[mode] >= len(datasets_cache[mode]):\n",
    "        indexes[mode] = 0\n",
    "        datasets_cache[mode] = load_data_cache(datasets[mode])\n",
    "\n",
    "    next_batch = datasets_cache[mode][indexes[mode]]\n",
    "    indexes[mode] += 1\n",
    "\n",
    "    return next_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\n",
      "[0.2   0.415 0.565 0.605 0.65  0.67 ]\n",
      "[tensor(1.6094, device='cuda:0') tensor(1.4893, device='cuda:0')\n",
      " tensor(1.3901, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(1.2903, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(1.1997, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(1.1635, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1997 0.4658 0.632  0.6494 0.688  0.7114]\n",
      "epoch: 20\n",
      "[0.195 0.56  0.755 0.755 0.77  0.785]\n",
      "[tensor(1.6159, device='cuda:0') tensor(1.3777, device='cuda:0')\n",
      " tensor(1.1775, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(1.0441, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.9421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.8674, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1969 0.6475 0.7734 0.8    0.8115 0.8076]\n",
      "epoch: 40\n",
      "[0.245 0.77  0.86  0.845 0.89  0.86 ]\n",
      "[tensor(1.6040, device='cuda:0') tensor(1.2470, device='cuda:0')\n",
      " tensor(1.0019, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.8322, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.7163, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.6211, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 60\n",
      "[0.155 0.7   0.88  0.89  0.89  0.875]\n",
      "[tensor(1.6146, device='cuda:0') tensor(1.1938, device='cuda:0')\n",
      " tensor(0.9225, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.7479, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.6506, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5958, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2124 0.7314 0.829  0.8394 0.843  0.8335]\n",
      "epoch: 80\n",
      "[0.205 0.855 0.9   0.955 0.915 0.925]\n",
      "[tensor(1.6158, device='cuda:0') tensor(1.1458, device='cuda:0')\n",
      " tensor(0.8471, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.6654, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5918, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5010, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1996 0.805  0.8604 0.867  0.8643 0.8613]\n",
      "epoch: 100\n",
      "[0.255 0.795 0.85  0.87  0.865 0.89 ]\n",
      "[tensor(1.6122, device='cuda:0') tensor(1.1307, device='cuda:0')\n",
      " tensor(0.8383, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.7175, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.6407, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5838, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 120\n",
      "[0.11  0.825 0.905 0.925 0.92  0.955]\n",
      "[tensor(1.6300, device='cuda:0') tensor(1.0735, device='cuda:0')\n",
      " tensor(0.7695, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.6193, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5027, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4303, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2087 0.8213 0.89   0.894  0.886  0.8887]\n",
      "epoch: 140\n",
      "[0.155 0.83  0.91  0.905 0.885 0.92 ]\n",
      "[tensor(1.6207, device='cuda:0') tensor(1.1084, device='cuda:0')\n",
      " tensor(0.8207, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.6793, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.6173, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5516, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2015 0.84   0.8936 0.9004 0.8965 0.8975]\n",
      "epoch: 160\n",
      "[0.125 0.87  0.915 0.925 0.93  0.91 ]\n",
      "[tensor(1.6351, device='cuda:0') tensor(1.0925, device='cuda:0')\n",
      " tensor(0.7856, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.6309, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5263, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5170, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 180\n",
      "[0.275 0.875 0.92  0.925 0.91  0.92 ]\n",
      "[tensor(1.5974, device='cuda:0') tensor(1.0007, device='cuda:0')\n",
      " tensor(0.7071, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4556, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4127, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2014 0.855  0.9116 0.912  0.913  0.9087]\n",
      "epoch: 200\n",
      "[0.24  0.905 0.95  0.975 0.97  0.985]\n",
      "[tensor(1.6048, device='cuda:0') tensor(0.9527, device='cuda:0')\n",
      " tensor(0.6797, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4911, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4192, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3451, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1957 0.858  0.9155 0.919  0.9185 0.9175]\n",
      "epoch: 220\n",
      "[0.185 0.85  0.9   0.935 0.945 0.94 ]\n",
      "[tensor(1.6099, device='cuda:0') tensor(0.9622, device='cuda:0')\n",
      " tensor(0.6949, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5428, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4575, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4094, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 240\n",
      "[0.16  0.825 0.925 0.935 0.945 0.95 ]\n",
      "[tensor(1.6210, device='cuda:0') tensor(0.9428, device='cuda:0')\n",
      " tensor(0.6769, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5293, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4359, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3620, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2029 0.868  0.9185 0.92   0.9204 0.9204]\n",
      "epoch: 260\n",
      "[0.31  0.84  0.93  0.905 0.955 0.95 ]\n",
      "[tensor(1.5841, device='cuda:0') tensor(0.8664, device='cuda:0')\n",
      " tensor(0.6080, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4852, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3956, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3394, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1973 0.869  0.9126 0.9146 0.915  0.9146]\n",
      "epoch: 280\n",
      "[0.17  0.88  0.945 0.935 0.95  0.925]\n",
      "[tensor(1.6410, device='cuda:0') tensor(0.9020, device='cuda:0')\n",
      " tensor(0.6379, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5068, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4389, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4436, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 300\n",
      "[0.195 0.93  0.94  0.94  0.965 0.945]\n",
      "[tensor(1.6240, device='cuda:0') tensor(0.8513, device='cuda:0')\n",
      " tensor(0.5664, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4525, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3985, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3533, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2017 0.8677 0.9136 0.9175 0.9185 0.922 ]\n",
      "epoch: 320\n",
      "[0.185 0.865 0.935 0.905 0.92  0.92 ]\n",
      "[tensor(1.6671, device='cuda:0') tensor(0.8832, device='cuda:0')\n",
      " tensor(0.6193, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5244, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4519, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4100, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1996 0.836  0.896  0.906  0.9097 0.913 ]\n",
      "epoch: 340\n",
      "[0.205 0.89  0.96  0.945 0.98  0.965]\n",
      "[tensor(1.6124, device='cuda:0') tensor(0.8756, device='cuda:0')\n",
      " tensor(0.6278, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5257, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4147, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3844, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 360\n",
      "[0.14  0.87  0.95  0.95  0.945 0.95 ]\n",
      "[tensor(1.7170, device='cuda:0') tensor(0.8311, device='cuda:0')\n",
      " tensor(0.5967, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4801, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3745, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2029 0.85   0.8984 0.9106 0.915  0.915 ]\n",
      "epoch: 380\n",
      "[0.125 0.885 0.96  0.945 0.955 0.95 ]\n",
      "[tensor(1.7068, device='cuda:0') tensor(0.7977, device='cuda:0')\n",
      " tensor(0.5478, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3591, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3021, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.211  0.836  0.8955 0.9062 0.9077 0.9106]\n",
      "epoch: 400\n",
      "[0.23  0.715 0.83  0.915 0.88  0.87 ]\n",
      "[tensor(1.6571, device='cuda:0') tensor(0.9120, device='cuda:0')\n",
      " tensor(0.7054, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5408, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4813, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4606, device='cuda:0', grad_fn=<DivBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 420\n",
      "[0.17  0.88  0.9   0.925 0.92  0.925]\n",
      "[tensor(1.7247, device='cuda:0') tensor(0.8341, device='cuda:0')\n",
      " tensor(0.6005, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4622, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4044, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3575, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1975 0.813  0.8833 0.8945 0.9033 0.903 ]\n",
      "epoch: 440\n",
      "[0.255 0.875 0.91  0.875 0.91  0.935]\n",
      "[tensor(1.6536, device='cuda:0') tensor(0.8024, device='cuda:0')\n",
      " tensor(0.6045, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5595, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4562, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4066, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2112 0.8086 0.8804 0.896  0.9014 0.9087]\n",
      "epoch: 460\n",
      "[0.125 0.85  0.845 0.88  0.895 0.875]\n",
      "[tensor(1.7265, device='cuda:0') tensor(0.8035, device='cuda:0')\n",
      " tensor(0.6042, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5245, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4576, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4717, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 480\n",
      "[0.25  0.82  0.905 0.915 0.93  0.945]\n",
      "[tensor(1.6700, device='cuda:0') tensor(0.7639, device='cuda:0')\n",
      " tensor(0.5140, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4299, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3615, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3498, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2004 0.813  0.876  0.8945 0.903  0.9067]\n",
      "epoch: 500\n",
      "[0.155 0.77  0.855 0.87  0.89  0.9  ]\n",
      "[tensor(1.8113, device='cuda:0') tensor(0.8115, device='cuda:0')\n",
      " tensor(0.6263, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4960, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4440, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4075, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2019 0.788  0.8584 0.8804 0.8916 0.902 ]\n",
      "epoch: 520\n",
      "[0.25  0.77  0.86  0.905 0.905 0.915]\n",
      "[tensor(1.7535, device='cuda:0') tensor(0.8518, device='cuda:0')\n",
      " tensor(0.6290, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4857, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4218, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4102, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 540\n",
      "[0.16  0.785 0.84  0.87  0.9   0.895]\n",
      "[tensor(1.8148, device='cuda:0') tensor(0.8104, device='cuda:0')\n",
      " tensor(0.6109, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4943, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3907, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3804, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.204  0.772  0.8525 0.8804 0.8896 0.8984]\n",
      "epoch: 560\n",
      "[0.215 0.8   0.775 0.835 0.865 0.885]\n",
      "[tensor(1.7645, device='cuda:0') tensor(0.8419, device='cuda:0')\n",
      " tensor(0.6759, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5617, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5165, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4437, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1962 0.7637 0.843  0.8716 0.8853 0.8955]\n",
      "epoch: 580\n",
      "[0.15  0.71  0.78  0.81  0.84  0.865]\n",
      "[tensor(1.8831, device='cuda:0') tensor(0.8710, device='cuda:0')\n",
      " tensor(0.7285, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.6226, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5636, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5068, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 600\n",
      "[0.295 0.815 0.87  0.925 0.875 0.915]\n",
      "[tensor(1.6352, device='cuda:0') tensor(0.7662, device='cuda:0')\n",
      " tensor(0.5249, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4178, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4171, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3524, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.196  0.7603 0.8433 0.8726 0.8843 0.893 ]\n",
      "epoch: 620\n",
      "[0.24  0.865 0.925 0.965 0.95  0.905]\n",
      "[tensor(1.8302, device='cuda:0') tensor(0.6928, device='cuda:0')\n",
      " tensor(0.4703, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3675, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3380, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4334, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2025 0.754  0.8394 0.873  0.886  0.898 ]\n",
      "epoch: 640\n",
      "[0.2   0.655 0.785 0.79  0.835 0.835]\n",
      "[tensor(1.8792, device='cuda:0') tensor(0.9060, device='cuda:0')\n",
      " tensor(0.6945, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.6367, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5444, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5538, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 660\n",
      "[0.17  0.845 0.87  0.92  0.905 0.94 ]\n",
      "[tensor(1.9059, device='cuda:0') tensor(0.7091, device='cuda:0')\n",
      " tensor(0.4900, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3602, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3455, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.2745, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1958 0.7437 0.8286 0.865  0.883  0.896 ]\n",
      "epoch: 680\n",
      "[0.245 0.75  0.9   0.925 0.95  0.965]\n",
      "[tensor(1.9022, device='cuda:0') tensor(0.7499, device='cuda:0')\n",
      " tensor(0.4962, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3427, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.2710, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.2268, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2042 0.724  0.816  0.856  0.8823 0.892 ]\n",
      "epoch: 700\n",
      "[0.21  0.845 0.885 0.905 0.92  0.925]\n",
      "[tensor(1.9321, device='cuda:0') tensor(0.6960, device='cuda:0')\n",
      " tensor(0.4448, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3646, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3390, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3015, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 720\n",
      "[0.22  0.725 0.845 0.855 0.92  0.915]\n",
      "[tensor(1.8989, device='cuda:0') tensor(0.7156, device='cuda:0')\n",
      " tensor(0.5017, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4004, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3010, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3019, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2    0.7104 0.8125 0.8516 0.873  0.885 ]\n",
      "epoch: 740\n",
      "[0.14  0.695 0.83  0.88  0.895 0.895]\n",
      "[tensor(2.2500, device='cuda:0') tensor(0.9205, device='cuda:0')\n",
      " tensor(0.5590, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4235, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4067, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3801, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2026 0.674  0.786  0.8374 0.8647 0.88  ]\n",
      "epoch: 760\n",
      "[0.19  0.74  0.825 0.855 0.895 0.87 ]\n",
      "[tensor(2.2212, device='cuda:0') tensor(0.8034, device='cuda:0')\n",
      " tensor(0.5709, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4956, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4027, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4137, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 780\n",
      "[0.25  0.725 0.88  0.925 0.915 0.92 ]\n",
      "[tensor(2.0888, device='cuda:0') tensor(0.7967, device='cuda:0')\n",
      " tensor(0.5044, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3653, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3601, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3032, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1979 0.679  0.8047 0.8496 0.872  0.8843]\n",
      "epoch: 800\n",
      "[0.23  0.71  0.88  0.875 0.895 0.92 ]\n",
      "[tensor(1.9970, device='cuda:0') tensor(0.8407, device='cuda:0')\n",
      " tensor(0.4528, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4487, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3627, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.2999, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1973 0.6787 0.8    0.852  0.8706 0.886 ]\n",
      "epoch: 820\n",
      "[0.24  0.665 0.82  0.85  0.89  0.87 ]\n",
      "[tensor(2.0321, device='cuda:0') tensor(0.8153, device='cuda:0')\n",
      " tensor(0.5787, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4749, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4146, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3761, device='cuda:0', grad_fn=<DivBackward0>)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 840\n",
      "[0.18  0.725 0.85  0.855 0.885 0.92 ]\n",
      "[tensor(2.1981, device='cuda:0') tensor(0.8159, device='cuda:0')\n",
      " tensor(0.5886, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5229, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4230, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3408, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2002 0.6772 0.7993 0.85   0.8755 0.8887]\n",
      "epoch: 860\n",
      "[0.21  0.64  0.78  0.815 0.855 0.885]\n",
      "[tensor(2.2732, device='cuda:0') tensor(0.9753, device='cuda:0')\n",
      " tensor(0.6762, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5062, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4421, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3863, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1943 0.6626 0.801  0.8506 0.88   0.894 ]\n",
      "epoch: 880\n",
      "[0.2   0.665 0.835 0.855 0.88  0.905]\n",
      "[tensor(2.4251, device='cuda:0') tensor(0.8371, device='cuda:0')\n",
      " tensor(0.4958, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4331, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3705, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3366, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 900\n",
      "[0.205 0.695 0.83  0.81  0.905 0.89 ]\n",
      "[tensor(2.4786, device='cuda:0') tensor(0.7719, device='cuda:0')\n",
      " tensor(0.5735, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4928, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3772, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3586, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.201  0.6543 0.7964 0.8535 0.878  0.894 ]\n",
      "epoch: 920\n",
      "[0.295 0.755 0.86  0.93  0.89  0.92 ]\n",
      "[tensor(2.2290, device='cuda:0') tensor(0.7875, device='cuda:0')\n",
      " tensor(0.4268, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3162, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3324, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3041, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.2097 0.6714 0.803  0.8574 0.884  0.897 ]\n",
      "epoch: 940\n",
      "[0.255 0.72  0.81  0.93  0.925 0.925]\n",
      "[tensor(2.2257, device='cuda:0') tensor(0.7644, device='cuda:0')\n",
      " tensor(0.5460, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3754, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3393, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3063, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "epoch: 960\n",
      "[0.165 0.68  0.785 0.815 0.86  0.885]\n",
      "[tensor(2.4708, device='cuda:0') tensor(0.8852, device='cuda:0')\n",
      " tensor(0.5876, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.5206, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.4029, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.3661, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1936 0.6665 0.793  0.854  0.8804 0.8965]\n",
      "epoch: 980\n",
      "[0.29  0.735 0.845 0.94  0.94  0.94 ]\n",
      "[tensor(2.2613, device='cuda:0') tensor(0.7343, device='cuda:0')\n",
      " tensor(0.4310, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.2609, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.2585, device='cuda:0', grad_fn=<DivBackward0>)\n",
      " tensor(0.2075, device='cuda:0', grad_fn=<DivBackward0>)]\n",
      "在mean process之前： (1000, 6)\n",
      "测试集准确率: [0.1963 0.667  0.8022 0.8584 0.8867 0.9023]\n"
     ]
    }
   ],
   "source": [
    "#=======================================模型\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from copy import deepcopy, copy\n",
    "\n",
    "#构建基学习器,学习一个任务\n",
    "class BaseNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BaseNet, self).__init__()\n",
    "        self.vars = nn.ParameterList()  ## 包含了所有需要被优化的tensor参数w和b\n",
    "        self.vars_bn = nn.ParameterList()  ##bn层参数\n",
    "\n",
    "        # 第1个conv2d=================================\n",
    "        # in_channels = 1, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 1, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        # 第1个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        self.vars_bn.extend([running_mean, running_var])   #标准化？\n",
    "\n",
    "        # 第2个conv2d====================================\n",
    "        # in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        # 第2个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    "\n",
    "        # 第3个conv2d====================================\n",
    "        # in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        # 第3个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    "\n",
    "        # 第4个conv2d======================================\n",
    "        # in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2\n",
    "        weight = nn.Parameter(torch.ones(64, 64, 3, 3))\n",
    "        nn.init.kaiming_normal_(weight)\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        # 第4个BatchNorm层\n",
    "        weight = nn.Parameter(torch.ones(64))\n",
    "        bias = nn.Parameter(torch.zeros(64))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "        running_mean = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        running_var = nn.Parameter(torch.zeros(64), requires_grad=False)\n",
    "        self.vars_bn.extend([running_mean, running_var])\n",
    "\n",
    "        ##linear===========================================\n",
    "        weight = nn.Parameter(torch.ones([5, 64]))\n",
    "        bias = nn.Parameter(torch.zeros(5))\n",
    "        self.vars.extend([weight, bias])\n",
    "\n",
    "    #         self.conv = nn.Sequential(\n",
    "    #             nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2),\n",
    "    #             nn.BatchNorm2d(64),\n",
    "    #             nn.ReLU(),\n",
    "    #             nn.MaxPool2d(2),\n",
    "\n",
    "    #             nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2),\n",
    "    #             nn.BatchNorm2d(64),\n",
    "    #             nn.ReLU(),\n",
    "    #             nn.MaxPool2d(2),\n",
    "\n",
    "    #             nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2),\n",
    "    #             nn.BatchNorm2d(64),\n",
    "    #             nn.ReLU(),\n",
    "    #             nn.MaxPool2d(2),\n",
    "\n",
    "    #             nn.Conv2d(in_channels = 64, out_channels = 64, kernel_size = (3,3), padding = 2, stride = 2),\n",
    "    #             nn.BatchNorm2d(64),\n",
    "    #             nn.ReLU(),\n",
    "    #             nn.MaxPool2d(2),\n",
    "\n",
    "    #             FlattenLayer(),\n",
    "    #             nn.Linear(64,5)\n",
    "    #         )\n",
    "\n",
    "    def forward(self, x, params=None, bn_training=True):\n",
    "        '''\n",
    "        :bn_training: set False to not update\n",
    "        :return:\n",
    "        '''\n",
    "        if params is None:\n",
    "            params = self.vars\n",
    "\n",
    "        weight, bias = params[0], params[1]  # 第1个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride=2, padding=2)\n",
    "        weight, bias = params[2], params[3]  # 第1个BN层\n",
    "        running_mean, running_var = self.vars_bn[0], self.vars_bn[1]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight, bias=bias, training=bn_training)\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # 第1个MAX_POOL层\n",
    "        x = F.relu(x, inplace=[True])  # 第1个relu\n",
    "\n",
    "        weight, bias = params[4], params[5]  # 第2个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride=2, padding=2)\n",
    "        weight, bias = params[6], params[7]  # 第2个BN层\n",
    "        running_mean, running_var = self.vars_bn[2], self.vars_bn[3]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight, bias=bias, training=bn_training)\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # 第2个MAX_POOL层\n",
    "        x = F.relu(x, inplace=[True])  # 第2个relu\n",
    "\n",
    "        weight, bias = params[8], params[9]  # 第3个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride=2, padding=2)\n",
    "        weight, bias = params[10], params[11]  # 第3个BN层\n",
    "        running_mean, running_var = self.vars_bn[4], self.vars_bn[5]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight, bias=bias, training=bn_training)\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # 第3个MAX_POOL层\n",
    "        x = F.relu(x, inplace=[True])  # 第3个relu\n",
    "\n",
    "        weight, bias = params[12], params[13]  # 第4个CONV层\n",
    "        x = F.conv2d(x, weight, bias, stride=2, padding=2)\n",
    "        x = F.relu(x, inplace=[True])  # 第4个relu\n",
    "        weight, bias = params[14], params[15]  # 第4个BN层\n",
    "        running_mean, running_var = self.vars_bn[6], self.vars_bn[7]\n",
    "        x = F.batch_norm(x, running_mean, running_var, weight=weight, bias=bias, training=bn_training)\n",
    "        x = F.max_pool2d(x, kernel_size=2)  # 第4个MAX_POOL层\n",
    "\n",
    "        x = x.view(x.size(0), -1)  ## flatten\n",
    "        weight, bias = params[16], params[17]  # linear\n",
    "        x = F.linear(x, weight, bias)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        return self.vars\n",
    "\n",
    "#构建元学习器，学习一个batch任务（8个任务为一个batch）\n",
    "class MetaLearner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MetaLearner, self).__init__()\n",
    "        self.update_step = 5  ## 基学习器训练更新5次\n",
    "        self.update_step_test = 5\n",
    "        self.net = BaseNet().cuda()\n",
    "        self.meta_lr = 2e-4  #元学习器的学习率\n",
    "        self.base_lr = 4 * 1e-2  #基学习器的学习率\n",
    "        self.inner_lr = 0.4\n",
    "        self.outer_lr = 1e-2\n",
    "        self.meta_optim = torch.optim.Adam(self.net.parameters(), lr=self.meta_lr)  #元学习器的优化器\n",
    "\n",
    "    def forward(self, x_spt, y_spt, x_qry, y_qry):  #输入一个batch的数据=8个任务的数据，每执行一次这个函数，元学习器就更新一次\n",
    "        x_spt, y_spt, x_qry, y_qry=x_spt.float(),y_spt.long(),x_qry.float(),y_qry.long()\n",
    "\n",
    "        #x_spt, y_spt, x_qry, y_qry=x_spt,torch.LongTensor(y_spt),torch.LongTensor(x_qry),torch.LongTensor(y_qry)\n",
    "        # 初始化\n",
    "        # 8         5    1    28  28\n",
    "        task_num, ways, shots, h, w = x_spt.size()\n",
    "        #      8，75，1，28，28\n",
    "        query_size = x_qry.size(1)  # 75 = 15 * 5\n",
    "        loss_list_qry = [0 for _ in range(self.update_step + 1)]   #[0,0,0,0,0,0]  #保存一个任务内，基学习器的初始损失加后面5次更新的损失\n",
    "        correct_list = [0 for _ in range(self.update_step + 1)]   #[0,0,0,0,0,0]  #保存一个任务内，基学习器的初始准确数加后面5次更新的准确数\n",
    "        #遍历一个batch内的每个任务，对每个任务训练更新基学习器5次\n",
    "        for i in range(task_num):\n",
    "            ## 一个任务上基学习器的第0步更新\n",
    "            y_hat = self.net(x_spt[i], params=None, bn_training=True)  # (ways * shots, ways) 用支持集，基学习器学习某一个任务\n",
    "\n",
    "            loss = F.cross_entropy(y_hat, y_spt[i]) #在支持集上，计算基学习器在某个任务的损失\n",
    "            grad = torch.autograd.grad(loss, self.net.parameters()) #计算该任务的梯度\n",
    "            tuples = zip(grad, self.net.parameters())  ## 将梯度和参数\\theta一一对应起来\n",
    "            # fast_weights这一步相当于求了一个\\theta - \\alpha*\\nabla(L)\n",
    "            fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], tuples))  #更新一次后的基学习器参数\n",
    "            # 在query集上测试，计算准确率\n",
    "            # 这一步使用更新前的数据\n",
    "            with torch.no_grad():\n",
    "                y_hat = self.net(x_qry[i], self.net.parameters(), bn_training=True) #使用前一步的基学习器计算查询集上的损失和准确数\n",
    "                loss_qry = F.cross_entropy(y_hat, y_qry[i])\n",
    "                loss_list_qry[0] += loss_qry\n",
    "                pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "                correct = torch.eq(pred_qry, y_qry[i]).sum().item()\n",
    "                correct_list[0] += correct\n",
    "\n",
    "            # 使用更新后的数据在query集上测试。\n",
    "            with torch.no_grad():\n",
    "                y_hat = self.net(x_qry[i], fast_weights, bn_training=True)  #使用更新后的基学习器计算查询集上的损失和准确数\n",
    "                loss_qry = F.cross_entropy(y_hat, y_qry[i])\n",
    "                loss_list_qry[1] += loss_qry\n",
    "                pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "                correct = torch.eq(pred_qry, y_qry[i]).sum().item()\n",
    "                correct_list[1] += correct\n",
    "\n",
    "            for k in range(1, self.update_step):   #再进行4次支持集上训练后更新基学习器的参数，并记录查询集上的验证损失和准确数\n",
    "                y_hat = self.net(x_spt[i], params=fast_weights, bn_training=True)\n",
    "                loss = F.cross_entropy(y_hat, y_spt[i])\n",
    "                grad = torch.autograd.grad(loss, fast_weights)\n",
    "                tuples = zip(grad, fast_weights)\n",
    "                fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], tuples))\n",
    "\n",
    "                y_hat = self.net(x_qry[i], params=fast_weights, bn_training=True)\n",
    "                loss_qry = F.cross_entropy(y_hat, y_qry[i])\n",
    "                loss_list_qry[k + 1] += loss_qry\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)\n",
    "                    correct = torch.eq(pred_qry, y_qry[i]).sum().item()\n",
    "                    correct_list[k + 1] += correct\n",
    "        #         print('hello')\n",
    "\n",
    "        #元学习器参数更新\n",
    "        loss_qry = loss_list_qry[-1] / task_num  #计算最后一次更新基学习器（第五次更新）上查询集的平均每个任务的损失（表示这个batch的损失）===》作为外部元学习器的一次损失，更新一次元学习器的参数\n",
    "        self.meta_optim.zero_grad()  # 梯度清零\n",
    "        loss_qry.backward() #\n",
    "        self.meta_optim.step()\n",
    "\n",
    "        #计算当前这步的元学习器学到的基学习器对于一个任务1+（5次更新）的对应的查询集平均准确率和损失\n",
    "        accs = np.array(correct_list) / (query_size * task_num)\n",
    "        loss = np.array(loss_list_qry) / (task_num)\n",
    "        return accs, loss  #返回每一步（基学习器在每个任务上更新共六步）的平均损失和准确数\n",
    "\n",
    "    #对MAML训练得到的基学习器测试准确率\n",
    "    def finetunning(self, x_spt, y_spt, x_qry, y_qry):  #输入一个任务的数据\n",
    "        assert len(x_spt.shape) == 4\n",
    "        x_spt, y_spt, x_qry, y_qry = x_spt.float(), y_spt.long(), x_qry.float(), y_qry.long()\n",
    "\n",
    "        query_size = x_qry.size(0)  #75\n",
    "        correct_list = [0 for _ in range(self.update_step_test + 1)]   #[0,0,0,0,0,0]\n",
    "\n",
    "        new_net = deepcopy(self.net)\n",
    "        y_hat = new_net(x_spt)\n",
    "        loss = F.cross_entropy(y_hat, y_spt)\n",
    "        grad = torch.autograd.grad(loss, new_net.parameters())\n",
    "        fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], zip(grad, new_net.parameters())))\n",
    "\n",
    "        # 在query集上测试，计算准确率\n",
    "        # 这一步使用更新前的基学习器参数\n",
    "        with torch.no_grad():\n",
    "            y_hat = new_net(x_qry, params=new_net.parameters(), bn_training=True)\n",
    "            pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "            correct = torch.eq(pred_qry, y_qry).sum().item()\n",
    "            correct_list[0] += correct\n",
    "\n",
    "        # 使用更新后的基学习器参数。\n",
    "        with torch.no_grad():\n",
    "            y_hat = new_net(x_qry, params=fast_weights, bn_training=True)\n",
    "            pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)  # size = (75)\n",
    "            correct = torch.eq(pred_qry, y_qry).sum().item()\n",
    "            correct_list[1] += correct\n",
    "\n",
    "        for k in range(1, self.update_step_test):  #五步更新基学习器（这里是剩下四步）\n",
    "            y_hat = new_net(x_spt, params=fast_weights, bn_training=True)\n",
    "            loss = F.cross_entropy(y_hat, y_spt)\n",
    "            grad = torch.autograd.grad(loss, fast_weights)\n",
    "            fast_weights = list(map(lambda p: p[1] - self.base_lr * p[0], zip(grad, fast_weights)))\n",
    "\n",
    "            y_hat = new_net(x_qry, fast_weights, bn_training=True)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                pred_qry = F.softmax(y_hat, dim=1).argmax(dim=1)\n",
    "                correct = torch.eq(pred_qry, y_qry).sum().item()\n",
    "                correct_list[k + 1] += correct\n",
    "\n",
    "        del new_net\n",
    "        accs = np.array(correct_list) / query_size  #返回一个任务上基学习器的平均准确率\n",
    "        return accs\n",
    "\n",
    "#模型训练\n",
    "import time\n",
    "device = torch.device('cuda')\n",
    "\n",
    "meta = MetaLearner().to(device)  #初始化元学习器\n",
    "\n",
    "epochs =1000 # 60000\n",
    "for step in range(epochs):\n",
    "    start = time.time()\n",
    "    x_spt, y_spt, x_qry, y_qry = next('train')  #取出一个batch的数据（包含8个任务）\n",
    "    x_spt, y_spt, x_qry, y_qry = torch.from_numpy(x_spt).to(device), \\\n",
    "                                 torch.from_numpy(y_spt).to(device), \\\n",
    "                                 torch.from_numpy(x_qry).to(device), \\\n",
    "                                 torch.from_numpy(y_qry).to(device)\n",
    "    accs, loss = meta(x_spt, y_spt, x_qry, y_qry)  #更新一次元学习器，返回这一次在八个任务上基学习器每一步更新得到的的损失和准确率\n",
    "    end = time.time()\n",
    "\n",
    "    if step % 20 == 0:  #每更新100次元学习器，输出一次当前的基学习器准确率和损失情况\n",
    "        print(\"epoch:\", step)\n",
    "        print(accs)\n",
    "        print(loss)\n",
    "\n",
    "    if step % 30 == 0:  #每更新1000次元学习器，从测试集分别取出一个batch数据，从中依此取出一个任务的数据测试当前学到的基学习器\n",
    "        accs = []\n",
    "        for _ in range(1000 // task_num):\n",
    "            # db_train.next('test')\n",
    "            x_spt, y_spt, x_qry, y_qry = next('test')\n",
    "            x_spt, y_spt, x_qry, y_qry = torch.from_numpy(x_spt).to(device), \\\n",
    "                                         torch.from_numpy(y_spt).to(device), \\\n",
    "                                         torch.from_numpy(x_qry).to(device), \\\n",
    "                                         torch.from_numpy(y_qry).to(device)\n",
    "\n",
    "            for x_spt_one, y_spt_one, x_qry_one, y_qry_one in zip(x_spt, y_spt, x_qry, y_qry):\n",
    "                test_acc = meta.finetunning(x_spt_one, y_spt_one, x_qry_one, y_qry_one)\n",
    "                accs.append(test_acc)\n",
    "        print('在mean process之前：', np.array(accs).shape)\n",
    "        accs = np.array(accs).mean(axis=0).astype(np.float16)\n",
    "        print('测试集准确率:', accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试集准确率: [0.2072 0.8696 0.9287 0.939  0.9414 0.942 ]  800个epoch，tju1/tju2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#测试集准确率: [0.2017 0.8677 0.9136 0.9175 0.9185 0.922 ]  1000个epoch tju1分割"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyenv] *",
   "language": "python",
   "name": "conda-env-pyenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
