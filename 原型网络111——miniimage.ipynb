{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "n_eposides_tr=5  #训练集迭代生成多少个任务\n",
    "n_eposides_va=10\n",
    "img_size=30  #处理后的数据集图片大小\n",
    "max_epoch=10\n",
    "save_epoch=2\n",
    "shot=1\n",
    "query=15\n",
    "train_way=30\n",
    "test_way=5\n",
    "save_path='./save_protonet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据集处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "ROOT_PATH = 'D:\\\\数据库\\\\小样本数据库\\\\mini-imagenet\\\\mini-imagenet'\n",
    "\n",
    "\n",
    "class MiniImageNet(Dataset):\n",
    "\n",
    "    def __init__(self, setname):\n",
    "        csv_path = osp.join(ROOT_PATH, setname + '.csv')\n",
    "        #filename                 label\n",
    "        #n0153282900000005.jpg    n01532829\n",
    "        lines = [x.strip() for x in open(csv_path, 'r').readlines()][1:]\n",
    "\n",
    "        data = []  #图片\n",
    "        label = []  #图片对应的类别索引\n",
    "        lb = -1\n",
    "\n",
    "        self.wnids = []  #记录该数据集都有哪些类，每个元素（类名）的索引对应label\n",
    "\n",
    "        for l in lines:\n",
    "            name, wnid = l.split(',')\n",
    "            path = osp.join(ROOT_PATH, 'images', name)  #每张图片的地址\n",
    "            if wnid not in self.wnids:\n",
    "                self.wnids.append(wnid)\n",
    "                lb += 1\n",
    "            data.append(path)\n",
    "            label.append(lb)\n",
    "\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        \n",
    "        #print(len(self.data))  #训练集一共38400个样本，测试集12000，验证集9600\n",
    "        #print(len(self.wnids)) #训练集包含64个类别，测试集20类，验证集16类，每类有600个样本\n",
    "        #print(self.data[0])  #self.data包含每个样本图片的地址  D:\\数据库\\小样本数据库\\mini-imagenet\\mini-imagenet\\images\\n0153282900000005.jpg\n",
    "        #print(self.label[0])  #self.label包含每个样本的类别 [0~63]\n",
    "\n",
    "\n",
    "        self.transform = transforms.Compose([  #图片转换\n",
    "            transforms.Resize(img_size),\n",
    "            transforms.CenterCrop(img_size),  #84\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        path, label = self.data[i], self.label[i]\n",
    "        image = self.transform(Image.open(path).convert('RGB'))\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class CategoriesSampler():\n",
    "\n",
    "    def __init__(self, label, n_batch, n_cls, n_per):\n",
    "        self.n_batch = n_batch\n",
    "        self.n_cls = n_cls  #每个任务包含的样本类别数\n",
    "        self.n_per = n_per  #每个任务每个类别包含的样本数=s+q\n",
    "\n",
    "        label = np.array(label)  #数据集的列别标签\n",
    "        self.m_ind = []  #按类别储存所有样本的索引\n",
    "        for i in range(max(label) + 1):  #遍历数据集中的所有类\n",
    "            ind = np.argwhere(label == i).reshape(-1)  #找出数据集中某个类别的样本索引\n",
    "            ind = torch.from_numpy(ind)\n",
    "            self.m_ind.append(ind)   #把每一个样本的索引按类别存放一起，后组成一个数组\n",
    "        #print(self.m_ind)  #[[类别一的600个样本的索引]，[类别二,,,,,,,[]]   \n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_batch  #有多少个batch\n",
    "    \n",
    "    def __iter__(self):  \n",
    "        for i_batch in range(self.n_batch):  #每次next(iter())生成一个eposide数据，1个eposide包含n_cls*（s+q）个样本,一个loader一共可以产生n_batch个eposide\n",
    "            batch = []\n",
    "            classes = torch.randperm(len(self.m_ind))[:self.n_cls]  #从数据集中随机选择n_cls个类\n",
    "            for c in classes:\n",
    "                l = self.m_ind[c]  #被选出的某个类的所有样本索引\n",
    "                pos = torch.randperm(len(l))[:self.n_per]  #从选出的每个类中随机取出n_per个样本\n",
    "                batch.append(l[pos])  \n",
    "            batch = torch.stack(batch).t().reshape(-1)  #batch保存一个eposide所需的所有样本的索引，注意.t()是按（s+q）个存放\n",
    "            yield batch  #返回一个n way k shot任务需要的样本，即n*(s+q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练集\n",
    "trainset = MiniImageNet('train')\n",
    "#每次生成100个batch=100个任务？每个batch包含支持集和查询集\n",
    "train_sampler = CategoriesSampler(trainset.label, n_eposides_tr,train_way, shot + query)\n",
    "train_loader = DataLoader(dataset=trainset, batch_sampler=train_sampler,num_workers=0, pin_memory=True)\n",
    "#每次迭代分别产生480个（样本vs标签），480=30*(15+1), iter(train_loader))[0]的shape为[480, 3, 84, 84])\n",
    "#print(len(next(iter(train_loader))[0]))  #480\n",
    "#print(len(iter(train_loader)))  #100\n",
    "\n",
    "#验证集\n",
    "valset = MiniImageNet('val')\n",
    "val_sampler = CategoriesSampler(valset.label, n_eposides_va,test_way, shot + query)\n",
    "val_loader = DataLoader(dataset=valset, batch_sampler=val_sampler,num_workers=0, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a=0\n",
    "# for i, batch in enumerate(train_loader, 1):\n",
    "#     a=i\n",
    "#     data, label = [_.cuda() for _ in batch]\n",
    "#     p = shot * train_way  #每个任务的支持集样本数\n",
    "#     data_shot, data_query = data[:p], data[p:]  #一个任务的支持集样本和查询集样本\n",
    "#     label_shot, label_query = label[:p], label[p:]\n",
    "#     #print(label)  #[16, 19, 59, 63, 20,,随机30个数，重复15遍]，所以前p个样本为支持集，后面的样本为查询集\n",
    "#     #print(len(data_shot))  #30\n",
    "#     #print(len(data)) #480\n",
    "#     #print(data_query[0].shape)  #([3, 28, 28])\n",
    "#     #print(len(label)) #450=30*15\n",
    "    \n",
    "# print(a) #10=n_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 工具函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import pprint\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def set_gpu(x):\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = x\n",
    "    print('using gpu:', x)\n",
    "\n",
    "\n",
    "def ensure_path(path):\n",
    "    if os.path.exists(path):\n",
    "        if input('{} exists, remove? ([y]/n)'.format(path)) != 'n':\n",
    "            shutil.rmtree(path)\n",
    "            os.makedirs(path)\n",
    "    else:\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "class Averager():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.v = 0\n",
    "\n",
    "    def add(self, x):\n",
    "        self.v = (self.v * self.n + x) / (self.n + 1)\n",
    "        self.n += 1\n",
    "\n",
    "    def item(self):\n",
    "        return self.v\n",
    "\n",
    "\n",
    "def count_acc(logits, label):\n",
    "    pred = torch.argmax(logits, dim=1)\n",
    "    return (pred == label).type(torch.cuda.FloatTensor).mean().item()\n",
    "\n",
    "\n",
    "def dot_metric(a, b):\n",
    "    return torch.mm(a, b.t())\n",
    "\n",
    "\n",
    "def euclidean_metric(a, b):\n",
    "    n = a.shape[0]\n",
    "    m = b.shape[0]\n",
    "    a = a.unsqueeze(1).expand(n, m, -1)\n",
    "    b = b.unsqueeze(0).expand(n, m, -1)\n",
    "    logits = -((a - b)**2).sum(dim=2)\n",
    "    return logits\n",
    "\n",
    "\n",
    "class Timer():\n",
    "\n",
    "    def __init__(self):\n",
    "        self.o = time.time()  #记录起始时间\n",
    "\n",
    "    def measure(self, p=1): #返回训练模型花费的时间\n",
    "        x = (time.time() - self.o) / p\n",
    "        x = int(x)\n",
    "        if x >= 3600:\n",
    "            return '{:.1f}h'.format(x / 3600)\n",
    "        if x >= 60:\n",
    "            return '{}m'.format(round(x / 60))\n",
    "        return '{}s'.format(x)\n",
    "\n",
    "_utils_pp = pprint.PrettyPrinter()\n",
    "def pprint(x):\n",
    "    _utils_pp.pprint(x)\n",
    "\n",
    "\n",
    "def l2_loss(pred, label):\n",
    "    return ((pred - label)**2).sum() / len(pred) / 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构造模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def conv_block(in_channels, out_channels):\n",
    "    bn = nn.BatchNorm2d(out_channels)\n",
    "    nn.init.uniform_(bn.weight) # 随机初始化bn层的参数\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "        bn,\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(2)\n",
    "    )\n",
    "\n",
    "\n",
    "class Convnet(nn.Module):\n",
    "\n",
    "    def __init__(self, x_dim=3, hid_dim=64, z_dim=64):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(  #84=》42=》21=》10=》5\n",
    "            conv_block(x_dim, hid_dim),  \n",
    "            conv_block(hid_dim, hid_dim),\n",
    "            conv_block(hid_dim, hid_dim),\n",
    "            conv_block(hid_dim, z_dim),  \n",
    "        )\n",
    "        self.out_channels = 1600  #1600=5*5*64  经过卷积运算后拉直的结果\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        return x.view(x.size(0), -1)   #拉直卷积运算后的特征shape为（？,1600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#实例化网络\n",
    "model = Convnet().cuda()\n",
    "    \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  #优化器\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)  #学习率调整器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把训练后的模型参数保存为args.save_path, name + '.pth'\n",
    "    def save_model(name):\n",
    "        torch.save(model.state_dict(), osp.join(save_path, name + '.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, train 1/5, loss=9.9470 acc=0.0467\n",
      "epoch 1, train 2/5, loss=6.7594 acc=0.0933\n",
      "epoch 1, train 3/5, loss=5.8546 acc=0.0400\n",
      "epoch 1, train 4/5, loss=4.9547 acc=0.0689\n",
      "epoch 1, train 5/5, loss=4.1162 acc=0.0756\n",
      "epoch 1, val, loss=1.6084 acc=0.2267\n",
      "ETA:17s/3m\n",
      "epoch 2, train 1/5, loss=4.2205 acc=0.0444\n",
      "epoch 2, train 2/5, loss=3.6415 acc=0.0867\n",
      "epoch 2, train 3/5, loss=3.7704 acc=0.0556\n",
      "epoch 2, train 4/5, loss=3.5180 acc=0.0711\n",
      "epoch 2, train 5/5, loss=3.4403 acc=0.0689\n",
      "epoch 2, val, loss=1.5972 acc=0.2587\n",
      "ETA:33s/3m\n",
      "epoch 3, train 1/5, loss=3.4601 acc=0.0733\n",
      "epoch 3, train 2/5, loss=3.4962 acc=0.0622\n",
      "epoch 3, train 3/5, loss=3.4852 acc=0.0467\n",
      "epoch 3, train 4/5, loss=3.4033 acc=0.0489\n",
      "epoch 3, train 5/5, loss=3.2918 acc=0.0622\n",
      "epoch 3, val, loss=1.5964 acc=0.2813\n",
      "ETA:51s/3m\n",
      "epoch 4, train 1/5, loss=3.2911 acc=0.0800\n",
      "epoch 4, train 2/5, loss=3.4070 acc=0.0556\n",
      "epoch 4, train 3/5, loss=3.3050 acc=0.0933\n",
      "epoch 4, train 4/5, loss=3.3226 acc=0.0778\n",
      "epoch 4, train 5/5, loss=3.3460 acc=0.0911\n",
      "epoch 4, val, loss=1.5939 acc=0.3093\n",
      "ETA:1m/3m\n",
      "epoch 5, train 1/5, loss=3.3427 acc=0.0800\n",
      "epoch 5, train 2/5, loss=3.3419 acc=0.0600\n",
      "epoch 5, train 3/5, loss=3.3628 acc=0.1022\n",
      "epoch 5, train 4/5, loss=3.3924 acc=0.0889\n",
      "epoch 5, train 5/5, loss=3.4131 acc=0.0556\n",
      "epoch 5, val, loss=1.6169 acc=0.2773\n",
      "ETA:1m/3m\n",
      "epoch 6, train 1/5, loss=3.2932 acc=0.0689\n",
      "epoch 6, train 2/5, loss=3.2312 acc=0.0822\n",
      "epoch 6, train 3/5, loss=3.3167 acc=0.0622\n",
      "epoch 6, train 4/5, loss=3.2982 acc=0.1022\n",
      "epoch 6, train 5/5, loss=3.2668 acc=0.0756\n",
      "epoch 6, val, loss=1.5631 acc=0.2613\n",
      "ETA:2m/3m\n",
      "epoch 7, train 1/5, loss=3.3216 acc=0.0711\n",
      "epoch 7, train 2/5, loss=3.3534 acc=0.0600\n",
      "epoch 7, train 3/5, loss=3.2430 acc=0.0800\n",
      "epoch 7, train 4/5, loss=3.2407 acc=0.1022\n",
      "epoch 7, train 5/5, loss=3.3319 acc=0.0889\n",
      "epoch 7, val, loss=1.5493 acc=0.3213\n",
      "ETA:2m/3m\n",
      "epoch 8, train 1/5, loss=3.2616 acc=0.0933\n",
      "epoch 8, train 2/5, loss=3.2568 acc=0.0911\n",
      "epoch 8, train 3/5, loss=3.3150 acc=0.0911\n",
      "epoch 8, train 4/5, loss=3.3265 acc=0.0844\n",
      "epoch 8, train 5/5, loss=3.3671 acc=0.0889\n",
      "epoch 8, val, loss=1.5366 acc=0.3387\n",
      "ETA:2m/3m\n",
      "epoch 9, train 1/5, loss=3.2436 acc=0.1156\n",
      "epoch 9, train 2/5, loss=3.2839 acc=0.1000\n",
      "epoch 9, train 3/5, loss=3.2747 acc=0.1067\n",
      "epoch 9, train 4/5, loss=3.1882 acc=0.1133\n",
      "epoch 9, train 5/5, loss=3.3287 acc=0.0778\n",
      "epoch 9, val, loss=1.6077 acc=0.2653\n",
      "ETA:3m/3m\n",
      "epoch 10, train 1/5, loss=3.3678 acc=0.0578\n",
      "epoch 10, train 2/5, loss=3.3177 acc=0.0800\n",
      "epoch 10, train 3/5, loss=3.3574 acc=0.0578\n",
      "epoch 10, train 4/5, loss=3.2720 acc=0.1000\n",
      "epoch 10, train 5/5, loss=3.2030 acc=0.1489\n",
      "epoch 10, val, loss=1.5342 acc=0.3013\n",
      "ETA:3m/3m\n"
     ]
    }
   ],
   "source": [
    "#记录训练每个batch的结果\n",
    "trlog = {}\n",
    "trlog['train_loss'] = []\n",
    "trlog['val_loss'] = []\n",
    "trlog['train_acc'] = []\n",
    "trlog['val_acc'] = []\n",
    "trlog['max_acc'] = 0.0  #记录模型最佳准确数\n",
    "\n",
    "timer = Timer()  #用于计算训练模型花费的时间\n",
    "                        #200+1\n",
    "for epoch in range(1, max_epoch+1):   #训练模型迭代200轮\n",
    "    lr_scheduler.step()  #初始化学习率调整器\n",
    "\n",
    "    #模型训练\n",
    "    model.train()\n",
    "\n",
    "    tl = Averager()  #计算训练时在每个任务损失和准确数的均值\n",
    "    ta = Averager()\n",
    "\n",
    "    for i, batch in enumerate(train_loader, 1):  #此循环会进行n_batch次，每次产生一个eposide所需的数据=(s+q)*cls\n",
    "        data, _ = [_.cuda() for _ in batch]   #分别把每个任务的数据data都搬到gpu上，data表示一个任务的数据=(s+q)*cls\n",
    "        p = shot * train_way  #每个任务的支持集样本数\n",
    "        data_shot, data_query = data[:p], data[p:]  #一个任务的支持集样本和查询集样本\n",
    "\n",
    "        proto = model(data_shot)  #输出（支持集样本数=shot*train_way，？*？*64）\n",
    "        proto = proto.reshape(shot, train_way, -1).mean(dim=0)  #计算支持集每类样本的样本均值作为该类的原型（train_way，？*？*64）\n",
    "\n",
    "        label = torch.arange(train_way).repeat(query)  #（train_way，q）  初始化查询集每个样本对应每个类别的分数\n",
    "        label = label.type(torch.cuda.LongTensor)\n",
    "\n",
    "        logits = euclidean_metric(model(data_query), proto)  #计算查询集每个样本到每个类别原型的欧式距离，（train_way*q）\n",
    "        loss = F.cross_entropy(logits, label)  #计算交叉熵损失\n",
    "        acc = count_acc(logits, label)  #计算预测准率\n",
    "        print('epoch {}, train {}/{}, loss={:.4f} acc={:.4f}'.format(epoch, i, len(train_loader), loss.item(), acc))\n",
    "\n",
    "        #把一个任务的损失和准确数加入到tl和ta，用于计算在当前所有任务上的损失和准确数的均值\n",
    "        tl.add(loss.item())\n",
    "        ta.add(acc)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        proto = None; logits = None; loss = None\n",
    "\n",
    "    #n_batch个episode上的平均损失和准确数\n",
    "    tl = tl.item()\n",
    "    ta = ta.item()\n",
    "\n",
    "    #模型验证，每n_batch个任务（train_loader走完一遍）训练完后，验证一次模型\n",
    "    model.eval()\n",
    "\n",
    "    vl = Averager()\n",
    "    va = Averager()\n",
    "    \n",
    "    for i, batch in enumerate(val_loader, 1):\n",
    "        data, _ = [_.cuda() for _ in batch]\n",
    "        p = shot * test_way\n",
    "        data_shot, data_query = data[:p], data[p:]\n",
    "        #with torch.no_grad():？？\n",
    "        proto = model(data_shot)\n",
    "        proto = proto.reshape(shot, test_way, -1).mean(dim=0)\n",
    "\n",
    "        label = torch.arange(test_way).repeat(query)\n",
    "        label = label.type(torch.cuda.LongTensor)\n",
    "\n",
    "        logits = euclidean_metric(model(data_query), proto)\n",
    "        loss = F.cross_entropy(logits, label)\n",
    "        acc = count_acc(logits, label)\n",
    "\n",
    "        vl.add(loss.item())\n",
    "        va.add(acc)\n",
    "            \n",
    "        proto = None; logits = None; loss = None\n",
    "\n",
    "    vl = vl.item()\n",
    "    va = va.item()\n",
    "    print('epoch {}, val, loss={:.4f} acc={:.4f}'.format(epoch, vl, va))\n",
    "\n",
    "    # 记录模型最佳\n",
    "    if va > trlog['max_acc']:\n",
    "        trlog['max_acc'] = va\n",
    "        save_model('max-acc')  #保存获得最佳准确率的模型\n",
    "\n",
    "    trlog['train_loss'].append(tl)\n",
    "    trlog['train_acc'].append(ta)\n",
    "    trlog['val_loss'].append(vl)\n",
    "    trlog['val_acc'].append(va)\n",
    "\n",
    "    torch.save(trlog, osp.join(save_path, 'trlog'))\n",
    "\n",
    "    save_model('epoch-last')  #保存最终的训练模型\n",
    "\n",
    "    if epoch % save_epoch == 0:  #每save_epoch个epoch保存一次模型\n",
    "        save_model('epoch-{}'.format(epoch))\n",
    "\n",
    "    print('ETA:{}/{}'.format(timer.measure(), timer.measure(epoch / max_epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch 1: 26.67(26.67)\n",
      "batch 2: 31.33(36.00)\n",
      "batch 3: 30.89(30.00)\n",
      "batch 4: 29.83(26.67)\n",
      "batch 5: 29.20(26.67)\n"
     ]
    }
   ],
   "source": [
    "load='./save_protonet/max-acc.pth'\n",
    "batch=5\n",
    "way=5\n",
    "shot=1\n",
    "query=30\n",
    "\n",
    "dataset = MiniImageNet('test')\n",
    "sampler = CategoriesSampler(dataset.label,batch, way, shot + query)\n",
    "loader = DataLoader(dataset, batch_sampler=sampler,num_workers=0, pin_memory=True)\n",
    "\n",
    "#导出训练得最佳准确数的模型\n",
    "model = Convnet().cuda()\n",
    "model.load_state_dict(torch.load(load))\n",
    "#模型验证\n",
    "model.eval()\n",
    "\n",
    "ave_acc = Averager()\n",
    "\n",
    "for i, batch in enumerate(loader, 1):\n",
    "    data, _ = [_.cuda() for _ in batch]\n",
    "    k = way * shot\n",
    "    data_shot, data_query = data[:k], data[k:]\n",
    "\n",
    "    x = model(data_shot)  #在支持集上前传的结果\n",
    "    x = x.reshape(shot, way, -1).mean(dim=0)  #计算支持集每类样本的均值==》原型\n",
    "    p = x\n",
    "\n",
    "    logits = euclidean_metric(model(data_query), p)\n",
    "\n",
    "    label = torch.arange(way).repeat(query)\n",
    "    label = label.type(torch.cuda.LongTensor)\n",
    "\n",
    "    acc = count_acc(logits, label)\n",
    "    ave_acc.add(acc)\n",
    "    print('batch {}: {:.2f}({:.2f})'.format(i, ave_acc.item() * 100, acc * 100))\n",
    "        \n",
    "    x = None; p = None; logits = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyenv] *",
   "language": "python",
   "name": "conda-env-pyenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
